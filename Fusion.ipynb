{"cells":[{"cell_type":"code","execution_count":null,"id":"4e9470f9","metadata":{"id":"4e9470f9"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"4f2423f1","metadata":{"id":"4f2423f1"},"outputs":[],"source":["train_labels = pd.read_csv(\"data/labels/train_split.csv\")\n","test_labels = pd.read_csv(\"data/labels/test_split.csv\")\n","dev_labels = pd.read_csv(\"data/labels/dev_split.csv\")"]},{"cell_type":"code","execution_count":null,"id":"4a4fadda","metadata":{"id":"4a4fadda"},"outputs":[],"source":["train_labels = train_labels.dropna(axis = 0).reset_index(drop = True)\n","test_labels = test_labels.dropna(axis = 0).reset_index(drop = True)\n","dev_labels = dev_labels.dropna(axis = 0).reset_index(drop = True)"]},{"cell_type":"code","execution_count":null,"id":"fd94c9c4","metadata":{"id":"fd94c9c4"},"outputs":[],"source":["def convert(df, df_name):\n","    labels = []\n","    for i in range(len(df)):\n","        temp = {}\n","        temp[\"ID\"] = df.loc[i, \"Participant_ID\"]\n","        temp[\"Gender\"] = df.loc[i, \"Gender\"]\n","        temp[\"Label\"] = df.loc[i, \"PHQ_Binary\"]\n","        temp[\"Score\"] = df.loc[i, \"PHQ_Score\"]\n","        temp[\"type\"] = df_name\n","        labels.append(temp)\n","    return labels"]},{"cell_type":"code","execution_count":null,"id":"10fd7aff","metadata":{"id":"10fd7aff"},"outputs":[],"source":["labels = []\n","labels = labels + convert(train_labels, \"train\") + convert(dev_labels, \"dev\") + convert(test_labels, \"test\")"]},{"cell_type":"code","execution_count":null,"id":"6b983f03","metadata":{"id":"6b983f03"},"outputs":[],"source":["labels_df = pd.DataFrame(labels, index = None)"]},{"cell_type":"code","execution_count":null,"id":"f26aff17","metadata":{"id":"f26aff17"},"outputs":[],"source":["labels_df = labels_df.sort_values(\"ID\")\n","labels_df = labels_df.reset_index(drop = True)\n","# labels_df = labels_df.drop(columns = [\"index\"])"]},{"cell_type":"code","execution_count":null,"id":"c2b8c370","metadata":{"id":"c2b8c370"},"outputs":[],"source":["def create_vocab(text, vocab, word_to_idx, idx_to_word, idx):\n","    for word in text.split(\" \"):\n","        if word not in vocab:\n","            vocab[word] = 1\n","            word_to_idx[word] = idx\n","            idx_to_word[idx] = word\n","            idx += 1\n","        else:\n","            vocab[word] += 1\n","    return vocab, word_to_idx, idx_to_word, idx"]},{"cell_type":"code","execution_count":null,"id":"e66f3b3e","metadata":{"id":"e66f3b3e"},"outputs":[],"source":["df_text = []\n","df_text_len = []\n","missed = []\n","vocab = {}\n","word_to_idx = {}\n","idx_to_word = {}\n","idx = 1\n","for i in range(len(labels_df)):\n","    file = labels_df.loc[i, \"ID\"]\n","    file_name = \"data/\" + str(file) + \"_transcript.csv\"\n","    try:\n","        text_df = pd.read_csv(file_name)\n","        text = \"\"\n","        for i in range(1, len(text_df), 2):\n","            text = text + text_df.loc[i, \"Start_Time,End_Time,Text,Confidence\"].split(\",\")[2]\n","        text = text.lower().strip()\n","        vocab, word_to_idx, idx_to_word, idx = create_vocab(text, vocab, word_to_idx, idx_to_word, idx)\n","        df_text.append(text)\n","        df_text_len.append(len(text.split(\" \")))\n","    except:\n","        missed.append(file)\n","        print(file)"]},{"cell_type":"code","execution_count":null,"id":"c7373ff3","metadata":{"id":"c7373ff3","outputId":"be76791b-e3cc-4183-be16-41d69c98ae56"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Gender</th>\n","      <th>Label</th>\n","      <th>Score</th>\n","      <th>type</th>\n","      <th>text</th>\n","      <th>text_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>300</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>dev</td>\n","      <td>interview in spanish good my parents are from ...</td>\n","      <td>170</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>301</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>dev</td>\n","      <td>okay i'm from los angeles i live in west los a...</td>\n","      <td>704</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>302</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>train</td>\n","      <td>when you're finished i'm fine how about yourse...</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>303</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>train</td>\n","      <td>when you're finished when she's done thank you...</td>\n","      <td>1049</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>304</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>train</td>\n","      <td>i miss is finished can you just let me know ho...</td>\n","      <td>544</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    ID  Gender  Label  Score   type  \\\n","0  300    male      0      2    dev   \n","1  301    male      0      3    dev   \n","2  302    male      0      4  train   \n","3  303  female      0      0  train   \n","4  304  female      0      6  train   \n","\n","                                                text  text_len  \n","0  interview in spanish good my parents are from ...       170  \n","1  okay i'm from los angeles i live in west los a...       704  \n","2  when you're finished i'm fine how about yourse...       295  \n","3  when you're finished when she's done thank you...      1049  \n","4  i miss is finished can you just let me know ho...       544  "]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["labels_df[\"text\"] = df_text\n","labels_df[\"text_len\"] = df_text_len\n","labels_df.head()"]},{"cell_type":"code","execution_count":null,"id":"356f718b","metadata":{"id":"356f718b","outputId":"3f16c3c4-dd19-4a8c-9052-402926386a6d"},"outputs":[{"data":{"text/plain":["12"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["len(labels_df[labels_df[\"Label\"] == 1])"]},{"cell_type":"code","execution_count":null,"id":"282754cd","metadata":{"id":"282754cd"},"outputs":[],"source":["import gensim\n","from gensim.models import Word2Vec, KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec"]},{"cell_type":"code","execution_count":null,"id":"82e7cb81","metadata":{"id":"82e7cb81"},"outputs":[],"source":["glove_path = \"data/glove.6B.100d.gz\"\n","pad = \"<pad>\"\n","pad_idx = 0"]},{"cell_type":"code","execution_count":null,"id":"44758008","metadata":{"id":"44758008"},"outputs":[],"source":["word_to_idx[pad] = pad_idx\n","idx_to_word[pad_idx] = pad"]},{"cell_type":"code","execution_count":null,"id":"eee221b6","metadata":{"id":"eee221b6"},"outputs":[],"source":["def dataset_creation(text):\n","    result = []\n","    global word_to_idx\n","    text = text.split(\" \")\n","    for word in text:\n","        result.append(word_to_idx[word])\n","    return result"]},{"cell_type":"code","execution_count":null,"id":"a343a8f3","metadata":{"id":"a343a8f3"},"outputs":[],"source":["indices = []\n","for i in range(len(labels_df)):\n","    indices.append(dataset_creation(labels_df.loc[i, \"text\"]))"]},{"cell_type":"code","execution_count":null,"id":"3724c2c1","metadata":{"id":"3724c2c1"},"outputs":[],"source":["labels_df['text_to_idx'] = indices"]},{"cell_type":"code","execution_count":null,"id":"50bf7c84","metadata":{"id":"50bf7c84","outputId":"d95c21a2-3b60-4e5f-8708-844eb282cd9d"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\nsuka\\AppData\\Local\\Temp/ipykernel_20316/1892350389.py:2: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n","  glove2word2vec(glove_path, glove_w2v_file)\n"]}],"source":["glove_w2v_file = 'data/glove.6B.100d.txt.word2vec'\n","glove2word2vec(glove_path, glove_w2v_file)\n","glove_vec = KeyedVectors.load_word2vec_format(glove_w2v_file)"]},{"cell_type":"code","execution_count":null,"id":"7af390d4","metadata":{"id":"7af390d4"},"outputs":[],"source":["weight_matrix = np.zeros((len(word_to_idx), 100), dtype = float)\n","embedding_dict = {}\n","embedding_dict[pad] = np.zeros(100, dtype = float)"]},{"cell_type":"code","execution_count":null,"id":"18818292","metadata":{"id":"18818292"},"outputs":[],"source":["def create_weight_matrix(weight_matrix, model):\n","    global word_to_idx, embedding_dict\n","    for word, idx in word_to_idx.items():\n","#         embed = np.zeros(100, dtype = float)\n","        if word in embedding_dict:\n","            weight_matrix[idx] = embedding_dict[word]\n","        else:\n","            try:\n","                weight_matrix[idx] = model[word]\n","            except KeyError:\n","                rand_embed = np.random.normal(scale = 0.6, size = (100,))\n","                weight_matrix[idx] = rand_embed\n","                embedding_dict[word] = rand_embed\n","    \n","    return weight_matrix"]},{"cell_type":"code","execution_count":null,"id":"8b1ad71a","metadata":{"id":"8b1ad71a"},"outputs":[],"source":["weight_matrix = create_weight_matrix(weight_matrix, glove_vec)"]},{"cell_type":"code","execution_count":null,"id":"c24379cd","metadata":{"id":"c24379cd","outputId":"4affbdb1-623b-42b5-e885-f705968b6f03"},"outputs":[{"data":{"text/plain":["(3236, 100)"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["weight_matrix.shape"]},{"cell_type":"code","execution_count":null,"id":"feaa600c","metadata":{"id":"feaa600c"},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"7e646413","metadata":{"id":"7e646413"},"outputs":[],"source":["labels_df[\"text_len\"].plot(kind = \"hist\", xlabel = None)"]},{"cell_type":"code","execution_count":null,"id":"40e1ca75","metadata":{"id":"40e1ca75"},"outputs":[],"source":["labels_df[labels_df[\"Gender\"] == \"male\"][\"text_len\"].plot(kind = \"hist\", color = \"green\", title = \"No.of words spoken by male patients\")"]},{"cell_type":"code","execution_count":null,"id":"b103cdd2","metadata":{"id":"b103cdd2"},"outputs":[],"source":["labels_df[labels_df[\"Gender\"] == \"female\"][\"text_len\"].plot(kind = \"hist\", color = \"red\", title = \"No.of words spoken by female patients\")"]},{"cell_type":"code","execution_count":null,"id":"deed4d13","metadata":{"id":"deed4d13"},"outputs":[],"source":["labels_df[labels_df[\"Label\"] == 0][\"text_len\"].plot(kind = \"hist\", color = \"green\", title = \"No.of words spoken by non-depressed patients\")"]},{"cell_type":"code","execution_count":null,"id":"9efff8b9","metadata":{"id":"9efff8b9"},"outputs":[],"source":["labels_df[labels_df[\"Label\"] == 1][\"text_len\"].plot(kind = \"hist\", color = \"red\", title = \"No.of words spoken by depressed patients\")"]},{"cell_type":"code","execution_count":null,"id":"adb5c8a5","metadata":{"id":"adb5c8a5"},"outputs":[],"source":["labels_df[labels_df[\"Gender\"] == \"female\"][\"Score\"].plot(kind = \"hist\", color = \"red\", title = \"Depression score for female patients\")"]},{"cell_type":"code","execution_count":null,"id":"e9a9f5ca","metadata":{"id":"e9a9f5ca"},"outputs":[],"source":["labels_df[labels_df[\"Gender\"] == \"male\"][\"Score\"].plot(kind = \"hist\", color = \"green\", title = \"Depression score for male patients\")"]},{"cell_type":"code","execution_count":null,"id":"6605762d","metadata":{"id":"6605762d"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"id":"0093a7d9","metadata":{"id":"0093a7d9"},"outputs":[],"source":["vocab_updated = {}\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","for word in list(vocab.keys()):\n","    word = word.lower()\n","    stem = stemmer.stem(word)\n","    lemma = lemmatizer.lemmatize(word)\n","    if word not in stop_words and stem not in stop_words and lemma not in stop_words:\n","        vocab_updated[word] = vocab[word]"]},{"cell_type":"code","execution_count":null,"id":"896312e7","metadata":{"id":"896312e7"},"outputs":[],"source":["from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"]},{"cell_type":"code","execution_count":null,"id":"7ec4dccc","metadata":{"id":"7ec4dccc"},"outputs":[],"source":["stopwords = set(STOPWORDS)"]},{"cell_type":"code","execution_count":null,"id":"24c3061e","metadata":{"id":"24c3061e"},"outputs":[],"source":["depressed_text = \" \".join(text for text in labels_df[labels_df[\"Label\"] == 1][\"text\"])"]},{"cell_type":"code","execution_count":null,"id":"097510ba","metadata":{"id":"097510ba"},"outputs":[],"source":["non_depressed_text = \" \".join(text for text in labels_df[labels_df[\"Label\"] == 0][\"text\"])"]},{"cell_type":"code","execution_count":null,"id":"004bdaf1","metadata":{"id":"004bdaf1"},"outputs":[],"source":["wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(depressed_text)"]},{"cell_type":"code","execution_count":null,"id":"d4055819","metadata":{"id":"d4055819"},"outputs":[],"source":["plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"ebedee4a","metadata":{"id":"ebedee4a"},"outputs":[],"source":["wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(non_depressed_text)"]},{"cell_type":"code","execution_count":null,"id":"09597532","metadata":{"id":"09597532"},"outputs":[],"source":["plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"2b5d1529","metadata":{"id":"2b5d1529"},"outputs":[],"source":["lengths = []\n","for text in labels_df[\"text\"]:\n","    temp = 0\n","    for word in text.split(\" \"):\n","        word = word.lower()\n","        stem = stemmer.stem(word)\n","        lemma = lemmatizer.lemmatize(word)\n","        if word not in stop_words and stem not in stop_words and lemma not in stop_words:\n","            temp += 1\n","    lengths.append(temp)"]},{"cell_type":"code","execution_count":null,"id":"5356f151","metadata":{"id":"5356f151"},"outputs":[],"source":["labels_df[\"text_len_no_stopwords\"] = lengths"]},{"cell_type":"code","execution_count":null,"id":"7928174e","metadata":{"id":"7928174e","outputId":"f73699b2-730d-44ab-fc9f-a22ca774adeb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Gender</th>\n","      <th>Label</th>\n","      <th>Score</th>\n","      <th>type</th>\n","      <th>text</th>\n","      <th>text_len</th>\n","      <th>text_to_idx</th>\n","      <th>text_len_no_stopwords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>300</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>dev</td>\n","      <td>interview in spanish good my parents are from ...</td>\n","      <td>170</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 10...</td>\n","      <td>85</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>301</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>dev</td>\n","      <td>okay i'm from los angeles i live in west los a...</td>\n","      <td>704</td>\n","      <td>[113, 23, 8, 114, 115, 10, 116, 2, 117, 114, 1...</td>\n","      <td>324</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>306</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>dev</td>\n","      <td>all right it's taking its time okay. and then ...</td>\n","      <td>1048</td>\n","      <td>[75, 27, 74, 59, 294, 295, 296, 21, 58, 124, 9...</td>\n","      <td>467</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>317</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>dev</td>\n","      <td>okay how was are you okay i'm okay a little sl...</td>\n","      <td>268</td>\n","      <td>[113, 100, 84, 7, 91, 113, 23, 113, 25, 224, 5...</td>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>320</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>dev</td>\n","      <td>and okay and then i will let you know okay now...</td>\n","      <td>366</td>\n","      <td>[21, 113, 21, 58, 10, 418, 179, 91, 92, 113, 2...</td>\n","      <td>163</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    ID  Gender  Label  Score type  \\\n","0  300    male      0      2  dev   \n","1  301    male      0      3  dev   \n","2  306  female      0      0  dev   \n","3  317    male      0      8  dev   \n","4  320  female      0     11  dev   \n","\n","                                                text  text_len  \\\n","0  interview in spanish good my parents are from ...       170   \n","1  okay i'm from los angeles i live in west los a...       704   \n","2  all right it's taking its time okay. and then ...      1048   \n","3  okay how was are you okay i'm okay a little sl...       268   \n","4  and okay and then i will let you know okay now...       366   \n","\n","                                         text_to_idx  text_len_no_stopwords  \n","0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 10...                     85  \n","1  [113, 23, 8, 114, 115, 10, 116, 2, 117, 114, 1...                    324  \n","2  [75, 27, 74, 59, 294, 295, 296, 21, 58, 124, 9...                    467  \n","3  [113, 100, 84, 7, 91, 113, 23, 113, 25, 224, 5...                    136  \n","4  [21, 113, 21, 58, 10, 418, 179, 91, 92, 113, 2...                    163  "]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["labels_df.head()"]},{"cell_type":"code","execution_count":null,"id":"cd855186","metadata":{"id":"cd855186"},"outputs":[],"source":["import torch\n","from sklearn.utils import class_weight\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error\n","from torch.utils.data import DataLoader, TensorDataset, Dataset\n","from torch.optim.lr_scheduler import StepLR\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"id":"98e244a4","metadata":{"id":"98e244a4"},"outputs":[],"source":["batch_size = 64"]},{"cell_type":"code","execution_count":null,"id":"ec6e2128","metadata":{"id":"ec6e2128"},"outputs":[],"source":["train_data = []\n","val_data = []\n","test_data = []\n","train_labels = []\n","val_labels = []\n","test_labels = []\n","train_score = []\n","val_score = []\n","test_score = []"]},{"cell_type":"code","execution_count":null,"id":"7a99bbac","metadata":{"id":"7a99bbac","outputId":"746f575a-28f5-4036-d1bb-9d2c08e2c49d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Gender</th>\n","      <th>Label</th>\n","      <th>Score</th>\n","      <th>type</th>\n","      <th>text</th>\n","      <th>text_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>300</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>dev</td>\n","      <td>interview in spanish good my parents are from ...</td>\n","      <td>170</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>301</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>dev</td>\n","      <td>okay i'm from los angeles i live in west los a...</td>\n","      <td>704</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>302</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>train</td>\n","      <td>when you're finished i'm fine how about yourse...</td>\n","      <td>295</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>303</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>train</td>\n","      <td>when you're finished when she's done thank you...</td>\n","      <td>1049</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>304</td>\n","      <td>female</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>train</td>\n","      <td>i miss is finished can you just let me know ho...</td>\n","      <td>544</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>268</th>\n","      <td>713</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>dev</td>\n","      <td>oh that's the one that was at the front desk o...</td>\n","      <td>335</td>\n","    </tr>\n","    <tr>\n","      <th>269</th>\n","      <td>715</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>test</td>\n","      <td>okay okay and please okay so so the beach shop...</td>\n","      <td>964</td>\n","    </tr>\n","    <tr>\n","      <th>270</th>\n","      <td>716</td>\n","      <td>male</td>\n","      <td>1</td>\n","      <td>15</td>\n","      <td>test</td>\n","      <td>for real i'm not a therapist okay inland empir...</td>\n","      <td>598</td>\n","    </tr>\n","    <tr>\n","      <th>271</th>\n","      <td>717</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>test</td>\n","      <td>once again after she says goodbye go ahead and...</td>\n","      <td>731</td>\n","    </tr>\n","    <tr>\n","      <th>272</th>\n","      <td>718</td>\n","      <td>male</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>test</td>\n","      <td>okay i'm doing okay it's friday i'm originally...</td>\n","      <td>852</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>273 rows Ã— 7 columns</p>\n","</div>"],"text/plain":["      ID  Gender  Label  Score   type  \\\n","0    300    male      0      2    dev   \n","1    301    male      0      3    dev   \n","2    302    male      0      4  train   \n","3    303  female      0      0  train   \n","4    304  female      0      6  train   \n","..   ...     ...    ...    ...    ...   \n","268  713    male      0      0    dev   \n","269  715    male      0      7   test   \n","270  716    male      1     15   test   \n","271  717    male      0      1   test   \n","272  718    male      0      3   test   \n","\n","                                                  text  text_len  \n","0    interview in spanish good my parents are from ...       170  \n","1    okay i'm from los angeles i live in west los a...       704  \n","2    when you're finished i'm fine how about yourse...       295  \n","3    when you're finished when she's done thank you...      1049  \n","4    i miss is finished can you just let me know ho...       544  \n","..                                                 ...       ...  \n","268  oh that's the one that was at the front desk o...       335  \n","269  okay okay and please okay so so the beach shop...       964  \n","270  for real i'm not a therapist okay inland empir...       598  \n","271  once again after she says goodbye go ahead and...       731  \n","272  okay i'm doing okay it's friday i'm originally...       852  \n","\n","[273 rows x 7 columns]"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["labels_df"]},{"cell_type":"code","execution_count":null,"id":"680cc46e","metadata":{"id":"680cc46e"},"outputs":[],"source":["for i in range(len(labels_df)):\n","    if labels_df.loc[i, \"type\"] == \"train\":\n","#         train_data.append(np.array(labels_df.loc[i, \"text_to_idx\"]))\n","        train_labels.append(labels_df.loc[i, \"Label\"])\n","        train_score.append(float(labels_df.loc[i, \"Score\"]))\n","    elif labels_df.loc[i, \"type\"] == \"dev\":\n","#         val_data.append(np.array(labels_df.loc[i, \"text_to_idx\"]))\n","        val_labels.append(labels_df.loc[i, \"Label\"])\n","        val_score.append(float(labels_df.loc[i, \"Score\"]))\n","    else:\n","#         test_data.append(np.array(labels_df.loc[i, \"text_to_idx\"]))\n","        test_labels.append(labels_df.loc[i, \"Label\"])\n","        test_score.append(float(labels_df.loc[i, \"Score\"]))"]},{"cell_type":"code","execution_count":null,"id":"ca33fd08","metadata":{"id":"ca33fd08","outputId":"c53bb302-f5e4-4674-ef89-89ae30df1dc9"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\nsuka\\AppData\\Local\\Temp/ipykernel_20316/1862688367.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  val_data = np.array(val_data)\n"]}],"source":["train_data = np.array(train_data)\n","val_data = np.array(val_data)\n","test_data = np.array(test_data)"]},{"cell_type":"code","execution_count":null,"id":"0a9d55ce","metadata":{"id":"0a9d55ce"},"outputs":[],"source":["train_labels = np.array(train_labels)\n","val_labels = np.array(val_labels)\n","test_labels = np.array(test_labels)\n","\n","train_score = np.array(train_score)\n","val_score = np.array(val_score)\n","test_score = np.array(test_score)\n","\n","# scaler = MinMaxScaler()\n","# train_score = np.expand_dims(np.array(train_score), axis = 1)\n","# val_score = np.expand_dims(np.array(val_score), axis = 1)\n","# test_score = np.expand_dims(np.array(test_score), axis = 1)\n","\n","# train_score = scaler.fit_transform(np.array(train_score))\n","# val_score = scaler.transform(np.array(val_score))\n","# test_score = scaler.transform(np.array(test_score))"]},{"cell_type":"code","execution_count":null,"id":"c12b3703","metadata":{"id":"c12b3703"},"outputs":[],"source":["class PadSequence:\n","    def __call__(self, batch):\n","        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n","        sequences = [torch.LongTensor(x[0]) for x in sorted_batch]\n","        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n","        lengths = torch.LongTensor([len(x) for x in sequences])\n","        labels = torch.LongTensor([x[1] for x in sorted_batch])\n","        return torch.LongTensor(sequences_padded), torch.LongTensor(labels), lengths"]},{"cell_type":"code","execution_count":null,"id":"ba23ebf4","metadata":{"id":"ba23ebf4"},"outputs":[],"source":["train_dataset = list(zip(train_data, train_labels))\n","val_dataset = list(zip(val_data, val_labels))\n","test_dataset = list(zip(test_data, test_labels))"]},{"cell_type":"code","execution_count":null,"id":"54abe881","metadata":{"id":"54abe881"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence())\n","val_loader = DataLoader(val_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence())\n","test_loader = DataLoader(test_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence())"]},{"cell_type":"code","execution_count":null,"id":"2c98b3e9","metadata":{"id":"2c98b3e9"},"outputs":[],"source":["input_dim = len(word_to_idx)\n","embed_dim = 100\n","hidden_dim = 256\n","linear_dim = 128\n","output_dim = 2"]},{"cell_type":"code","execution_count":null,"id":"d35a8347","metadata":{"id":"d35a8347"},"outputs":[],"source":["def create_embedding(input_dim, embed_dim, pad_idx, weight_matrix):\n","    weight_matrix = torch.FloatTensor(weight_matrix).to(device)\n","    embedding = torch.nn.Embedding(num_embeddings = input_dim, embedding_dim = embed_dim, padding_idx = pad_idx)\n","    embedding.load_state_dict({'weight': weight_matrix})\n","    return embedding"]},{"cell_type":"code","execution_count":null,"id":"e1e6b4f2","metadata":{"id":"e1e6b4f2"},"outputs":[],"source":["def accuracy(pred, targ):\n","    pred = pred.argmax(dim = 1, keepdim = True)\n","    correct = pred.squeeze(1).eq(targ)\n","    return correct.sum() / torch.FloatTensor([targ.shape[0]]).to(device)"]},{"cell_type":"code","execution_count":null,"id":"99962c27","metadata":{"id":"99962c27"},"outputs":[],"source":["class glove_GRU(torch.nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx, weight_matrix):\n","        super(glove_GRU, self).__init__()\n","        self.pad_idx = pad_idx\n","        self.linear_dim = linear_dim\n","        self.embedding = create_embedding(input_dim, embed_dim, pad_idx, weight_matrix)\n","        self.gru = torch.nn.GRU(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 1, bidirectional = False, batch_first = True)\n","#         self.dropout = torch.nn.Dropout(p = 0.33)\n","        self.linear = torch.nn.Linear(hidden_dim, linear_dim)\n","        self.relu = torch.nn.ReLU()\n","        self.classifier = torch.nn.Linear(linear_dim, output_dim)\n","    \n","    def forward(self, x, lengths):\n","        emb = self.embedding(x)\n","        packed = pack_padded_sequence(emb, lengths, batch_first = True, enforce_sorted = True)\n","        gru_out, h_t = self.gru(packed)\n","#         gru_out = self.dropout(gru_out[:, -1])\n","#         gru_out = gru_out[:, -1]\n","        lin_out = self.linear(h_t[-1])\n","        class_out = self.classifier(lin_out)\n","        return class_out\n","    \n","    def init_weights(self):\n","        for name, param in self.named_parameters():\n","            torch.nn.init.normal_(param.data, mean=0, std=0.1)"]},{"cell_type":"code","execution_count":null,"id":"73118394","metadata":{"id":"73118394"},"outputs":[],"source":["model = glove_GRU(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx, weight_matrix).to(device)\n","# model.init_weights()\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"66a49cf6","metadata":{"id":"66a49cf6"},"outputs":[],"source":["class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n","# class_weights[1] = 1.05 * class_weights[1]\n","class_weights"]},{"cell_type":"code","execution_count":null,"id":"d00acbc6","metadata":{"id":"d00acbc6"},"outputs":[],"source":["class_weights = torch.FloatTensor(class_weights)\n","loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n","# scheduler = StepLR(optimizer, step_size = 25) "]},{"cell_type":"code","execution_count":null,"id":"6f2b6a99","metadata":{"id":"6f2b6a99"},"outputs":[],"source":["epochs = 20\n","dev_max_acc = 0\n","for epoch in range(epochs):\n","    model.train()\n","    train_acc = 0.0\n","    dev_acc = 0.0\n","    batch = 0\n","    for inputs, target, lens in train_loader:\n","        print(batch, end = \"\\r\")\n","        batch += 1\n","        inputs, target = inputs.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(inputs, lens)\n","        output = output.view(-1, output.shape[-1])\n","        target = target.view(-1)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += float(accuracy(output, target).item())\n","    model.eval()\n","    for inputs, target, lens in val_loader:\n","        inputs, target = inputs.to(device), target.to(device)\n","        inputs, target = inputs, target\n","        output = model(inputs, lens)\n","        output = output.view(-1, output.shape[-1])\n","        target = target.view(-1)\n","        loss = loss_fn(output, target)\n","        dev_acc += float(accuracy(output, target).item())\n","    \n","    train_acc = (train_acc * batch_size)/len(train_loader.dataset)\n","    dev_acc = (dev_acc * batch_size)/len(val_loader.dataset)\n","    \n","    print('Epoch: {} \\tTraining Acc: {:.6f} \\tDev Set Acc: {:.6f}'.format(epoch+1, train_acc,dev_acc))\n","    if dev_acc >= dev_max_acc:\n","        print('Dev Set acc increased ({:.6f} --> {:.6f}). Saving model...'.format(dev_max_acc, dev_acc))\n","        torch.save(model.state_dict(), 'model/text_GRU_alternate.pt')\n","        dev_max_acc = dev_acc "]},{"cell_type":"code","execution_count":null,"id":"d77184f3","metadata":{"id":"d77184f3"},"outputs":[],"source":["del model, optimizer, loss_fn"]},{"cell_type":"code","execution_count":null,"id":"fa4ea843","metadata":{"id":"fa4ea843"},"outputs":[],"source":["def concordance_correlation_coefficient(y_true, y_pred,\n","                       sample_weight=None,\n","                       multioutput='uniform_average'):\n","    cor=np.corrcoef(y_true,y_pred)[0][1]\n","#     print(\"Corrcoef: \", cor)\n","    mean_true=np.mean(y_true)\n","#     print(\"Target mean: \", mean_true)\n","    mean_pred=np.mean(y_pred)\n","#     print(\"Pred mean: \", mean_pred)\n","    var_true=np.var(y_true)\n","    var_pred=np.var(y_pred)\n","    \n","    sd_true=np.std(y_true)\n","#     print(\"Target std: \", sd_true)\n","    sd_pred=np.std(y_pred)\n","#     print(\"Pred std: \", sd_pred)\n","#     print(\"Value: \", 2 * cor * sd_true * sd_pred)\n","    numerator=2*cor*sd_true*sd_pred\n","    \n","    denominator=var_true+var_pred+(mean_true-mean_pred)**2\n","#     print(\"Num: \", numerator, \"\\nDenom: \", denominator)\n","    return numerator/denominator\n"]},{"cell_type":"code","execution_count":null,"id":"9bdce81e","metadata":{"id":"9bdce81e"},"outputs":[],"source":["model = glove_GRU(input_dim, embed_dim, hidden_dim, linear_dim, output_dim, pad_idx, weight_matrix).to(device)\n","model.load_state_dict(torch.load(\"model/text_GRU_alternate.pt\"))\n","\n","model.eval()\n","targets = None\n","preds = None\n","with torch.no_grad():\n","    for inputs, target, lens in test_loader:\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = model(inputs, lens)\n","        output = output.view(-1, output.shape[-1])\n","        target = target.view(-1)\n","        _, pred = torch.max(output, 1)\n","#         print(target.shape, pred.shape)\n","        if targets == None:\n","            targets = target\n","            preds = pred\n","        else:\n","            targets = torch.cat((targets, target), 0)\n","            preds = torch.cat((preds, pred), 0)\n","targets, preds = targets.cpu().detach().numpy(), preds.cpu().detach().numpy()\n","print(\"F1: \", f1_score(targets, preds, average = \"macro\"))\n","print(\"Accuracy: \", accuracy_score(targets, preds))\n","# print(\"CCC: \", concordance_correlation_coefficient(targets, preds))\n","cm = confusion_matrix(targets, preds)\n","cmd_obj = ConfusionMatrixDisplay(cm, display_labels=np.unique(train_labels))\n","cmd_obj.plot()\n","cmd_obj.ax_.set(title='Confusion Matrix', xlabel='Predicted Labels', ylabel='True Labels')\n","plt.show()\n","\n","del model"]},{"cell_type":"code","execution_count":null,"id":"21b5c8f6","metadata":{"id":"21b5c8f6"},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"code","execution_count":null,"id":"80dab90f","metadata":{"id":"80dab90f"},"outputs":[],"source":["class PadSequence_regression:\n","    def __call__(self, batch):\n","        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n","        sequences = [torch.LongTensor(x[0]) for x in sorted_batch]\n","        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n","        lengths = torch.LongTensor([len(x) for x in sequences])\n","        scores = torch.FloatTensor([x[1] for x in sorted_batch])\n","        return torch.LongTensor(sequences_padded), torch.FloatTensor(scores), lengths"]},{"cell_type":"code","execution_count":null,"id":"01b44f53","metadata":{"id":"01b44f53"},"outputs":[],"source":["train_dataset_reg = list(zip(train_data, train_score))\n","val_dataset_reg = list(zip(val_data, val_score))\n","test_dataset_reg = list(zip(test_data, test_score))"]},{"cell_type":"code","execution_count":null,"id":"6b32da63","metadata":{"id":"6b32da63"},"outputs":[],"source":["train_loader_reg = DataLoader(train_dataset_reg, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression())\n","val_loader_reg = DataLoader(val_dataset_reg, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression())\n","test_loader_reg = DataLoader(test_dataset_reg, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression())"]},{"cell_type":"code","execution_count":null,"id":"111d5c09","metadata":{"id":"111d5c09"},"outputs":[],"source":["input_dim = len(word_to_idx)\n","embed_dim = 100\n","hidden_dim = 256\n","linear_dim_1 = 128\n","linear_dim_2 = linear_dim_1 // 2\n","# linear_dim_3 = linear_dim_2"]},{"cell_type":"code","execution_count":null,"id":"8f72d39a","metadata":{"id":"8f72d39a"},"outputs":[],"source":["max_split_size_mb = 128\n","\n","PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb"]},{"cell_type":"code","execution_count":null,"id":"b4730689","metadata":{"id":"b4730689"},"outputs":[],"source":["print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n","print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n","print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"]},{"cell_type":"code","execution_count":null,"id":"d71d44e8","metadata":{"id":"d71d44e8"},"outputs":[],"source":["class glove_GRU_regression(torch.nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, linear_dim_1, linear_dim_2, pad_idx, weight_matrix):\n","        super(glove_GRU_regression, self).__init__()\n","        self.pad_idx = pad_idx\n","#         self.linear_dim = linear_dim\n","        self.embedding = create_embedding(input_dim, embed_dim, pad_idx, weight_matrix)\n","        self.gru = torch.nn.GRU(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 2, bidirectional = False, batch_first = True, dropout = 0)\n","        self.dropout = torch.nn.Dropout(p = 0)\n","        self.linear_1 = torch.nn.Linear(hidden_dim, linear_dim_1)\n","        self.linear_2 = torch.nn.Linear(linear_dim_1, linear_dim_2)\n","#         self.linear_3 = torch.nn.Linear(linear_dim_2, linear_dim_3)\n","#         self.relu = torch.nn.ReLU()\n","        self.output = torch.nn.Linear(linear_dim_2, 1)\n","    \n","    def forward(self, x, lengths):\n","        emb = self.embedding(x)\n","#         packed = pack_padded_sequence(emb, lengths, batch_first = True, enforce_sorted = True)\n","        gru_out, h_t = self.gru(emb)\n","#         gru_out = self.dropout(gru_out[:, -1])\n","        gru_out = gru_out[:, -1, :]\n","        lin_out_1 = self.dropout(self.linear_1(self.dropout(gru_out)))\n","        lin_out_2 = self.dropout(self.linear_2(lin_out_1))\n","#         lin_out_3 = self.dropout(self.linear_3(lin_out_2))\n","        out = self.output(lin_out_2)\n","        return out"]},{"cell_type":"code","execution_count":null,"id":"62816092","metadata":{"id":"62816092"},"outputs":[],"source":["model = glove_GRU_regression(input_dim, embed_dim, hidden_dim, linear_dim_1, linear_dim_2, pad_idx, weight_matrix).to(device)\n","# model.init_weights()\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"6255d019","metadata":{"id":"6255d019"},"outputs":[],"source":["loss_fn = torch.nn.MSELoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-6)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1, last_epoch=- 1, verbose=True)"]},{"cell_type":"code","execution_count":null,"id":"60fcbf11","metadata":{"id":"60fcbf11"},"outputs":[],"source":["epochs = 30\n","dev_min_loss = np.inf\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    dev_loss = 0.0\n","    batch = 0\n","    for inputs, target, lens in train_loader_reg:\n","        print(batch, end = \"\\r\")\n","        batch += 1\n","        inputs, target = inputs.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(inputs, lens)\n","        output = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        loss = torch.sqrt(loss_fn(output, target))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","    model.eval()\n","    for inputs, target, lens in val_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        inputs, target = inputs, target\n","        output = model(inputs, lens)\n","        output = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        loss = torch.sqrt(loss_fn(output, target))\n","        dev_loss += loss.item()\n","    \n","    train_loss = train_loss / len(train_loader_reg)\n","    dev_loss = dev_loss / len(val_loader_reg)\n","#     scheduler.step()\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tDev Set Loss: {:.6f}'.format(epoch+1, train_loss,dev_loss))\n","    if dev_loss <= dev_min_loss:\n","        print('Dev Set Loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(dev_min_loss, dev_loss))\n","        torch.save(model.state_dict(), 'model/text_GRU_reg_alternate.pt')\n","        dev_min_loss = dev_loss "]},{"cell_type":"code","execution_count":null,"id":"cd10d187","metadata":{"id":"cd10d187"},"outputs":[],"source":["del model, loss_fn, optimizer"]},{"cell_type":"code","execution_count":null,"id":"6b9d4ce5","metadata":{"id":"6b9d4ce5"},"outputs":[],"source":["model = glove_GRU_regression(input_dim, embed_dim, hidden_dim, linear_dim_1, linear_dim_2, pad_idx, weight_matrix).to(device)\n","model.load_state_dict(torch.load(\"model/text_GRU_reg_alternate.pt\"))\n","model.eval()\n","targets = None\n","with torch.no_grad():\n","    for inputs, target, lens in test_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = model(inputs, lens)\n","        pred = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        if targets == None:\n","            targets = target\n","            preds = pred\n","        else:\n","            targets = torch.cat((targets, target), 0)\n","            preds = torch.cat((preds, pred), 0)\n","targets, preds = targets.cpu().detach().numpy(), preds.cpu().detach().numpy()\n","# targets = scaler.inverse_transform(np.expand_dims(targets, axis = 1))\n","# preds = scaler.inverse_transform(np.expand_dims(preds, axis = 1))\n","targets = np.expand_dims(targets, axis = 1)\n","preds = np.expand_dims(preds, axis = 1)\n","print(\"RMSE Score: \", mean_squared_error(targets, preds, squared = False))\n","targets, preds = np.squeeze(targets), np.squeeze(preds)\n","# print(\"Accuracy: \", accuracy_score(targets, preds))\n","print(\"CCC: \", concordance_correlation_coefficient(targets, preds))\n","# cm = confusion_matrix(targets, preds)\n","# cmd_obj = ConfusionMatrixDisplay(cm, display_labels=np.unique(train_labels))\n","# cmd_obj.plot()\n","# cmd_obj.ax_.set(title='Confusion Matrix', xlabel='Predicted Labels', ylabel='True Labels')\n","# plt.show()\n","# print(\"\\n\", targets)\n","# print(preds)\n","print()\n","del model"]},{"cell_type":"code","execution_count":null,"id":"3f1c2a72","metadata":{"id":"3f1c2a72"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer"]},{"cell_type":"code","execution_count":null,"id":"1dddc9bb","metadata":{"id":"1dddc9bb"},"outputs":[],"source":["transformer = SentenceTransformer(\"all-mpnet-base-v2\", device = 'cuda')"]},{"cell_type":"code","execution_count":null,"id":"405116c2","metadata":{"id":"405116c2"},"outputs":[],"source":["def create_tranformer_dataset(transcript, model):\n","    encoding = []\n","    for i in range(1, len(transcript), 2):\n","        encoding.append(model.encode(transcript.loc[i, \"Start_Time,End_Time,Text,Confidence\"].split(\",\")[2]))\n","    return np.array(encoding)"]},{"cell_type":"code","execution_count":null,"id":"87011bc5","metadata":{"id":"87011bc5"},"outputs":[],"source":["sbert_train = []\n","sbert_val = []\n","sbert_test = []\n","for i in range(len(labels_df)):\n","    file = labels_df.loc[i, \"ID\"]\n","    file_name = \"data/\" + str(file) + \"_transcript.csv\"\n","    df = pd.read_csv(file_name)\n","    data = create_tranformer_dataset(df, transformer)\n","    if labels_df.loc[i, \"type\"] == \"train\":\n","        sbert_train.append(data)\n","    elif labels_df.loc[i, \"type\"] == \"dev\":\n","        sbert_val.append(data)\n","    else:\n","        sbert_test.append(data) "]},{"cell_type":"code","execution_count":null,"id":"aa99703b","metadata":{"id":"aa99703b"},"outputs":[],"source":["class PadSequence_sbert:\n","    def __call__(self, batch):\n","        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n","        sequences = [torch.FloatTensor(x[0]) for x in sorted_batch]\n","        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)\n","        lengths = torch.LongTensor([len(x) for x in sequences])\n","        labels = torch.LongTensor([x[1] for x in sorted_batch])\n","        return torch.FloatTensor(sequences_padded), torch.LongTensor(labels), lengths"]},{"cell_type":"code","execution_count":null,"id":"50b89299","metadata":{"id":"50b89299","outputId":"2a1ad272-06aa-4711-e5fc-32b80186397e"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\nsuka\\AppData\\Local\\Temp/ipykernel_20316/2266321980.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  sbert_train, sbert_val, sbert_test = np.array(sbert_train), np.array(sbert_val), np.array(sbert_test)\n"]}],"source":["sbert_train, sbert_val, sbert_test = np.array(sbert_train), np.array(sbert_val), np.array(sbert_test)"]},{"cell_type":"code","execution_count":null,"id":"992cb270","metadata":{"id":"992cb270"},"outputs":[],"source":["sbert_train_dataset = list(zip(sbert_train, train_labels))\n","sbert_val_dataset = list(zip(sbert_val, val_labels))\n","sbert_test_dataset = list(zip(sbert_test, test_labels))"]},{"cell_type":"code","execution_count":null,"id":"be941037","metadata":{"id":"be941037"},"outputs":[],"source":["batch_size = 64"]},{"cell_type":"code","execution_count":null,"id":"b9623bfc","metadata":{"id":"b9623bfc"},"outputs":[],"source":["sbert_train_loader = DataLoader(sbert_train_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_sbert())\n","sbert_val_loader = DataLoader(sbert_val_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_sbert())\n","sbert_test_loader = DataLoader(sbert_test_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_sbert())"]},{"cell_type":"code","execution_count":null,"id":"479842af","metadata":{"id":"479842af"},"outputs":[],"source":["class sbert_GRU(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, linear_dim, output_dim):\n","        super(sbert_GRU, self).__init__()\n","        self.linear_dim = linear_dim\n","        self.gru = torch.nn.GRU(input_size = input_dim, hidden_size = hidden_dim, num_layers = 1, bidirectional = False, batch_first = True)\n","        self.dropout = torch.nn.Dropout(p = 0.33)\n","        self.linear = torch.nn.Linear(hidden_dim, linear_dim)\n","        self.relu = torch.nn.ReLU()\n","        self.classifier = torch.nn.Linear(linear_dim, output_dim)\n","    \n","    def forward(self, x, lengths):\n","        packed = pack_padded_sequence(x, lengths, batch_first = True, enforce_sorted = True)\n","        gru_out, h_t = self.gru(packed)\n","#         gru_out = self.dropout(gru_out[:, -1])\n","#         gru_out = gru_out[:, -1]\n","        lin_out = self.linear(h_t[-1])\n","        class_out = self.classifier(lin_out)\n","        return class_out\n","    \n","    def init_weights(self):\n","        for name, param in self.named_parameters():\n","            torch.nn.init.normal_(param.data, mean=0, std=0.1)"]},{"cell_type":"code","execution_count":null,"id":"c41968d7","metadata":{"id":"c41968d7"},"outputs":[],"source":["input_dim = 768\n","hidden_dim = 1024\n","linear_dim = 512\n","output_dim = 2"]},{"cell_type":"code","execution_count":null,"id":"a0025292","metadata":{"id":"a0025292","outputId":"6fb8881c-5d5d-49fe-be0d-b3853ca854af"},"outputs":[{"name":"stdout","output_type":"stream","text":["sbert_GRU(\n","  (gru): GRU(768, 1024, batch_first=True)\n","  (dropout): Dropout(p=0.33, inplace=False)\n","  (linear): Linear(in_features=1024, out_features=512, bias=True)\n","  (relu): ReLU()\n","  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",")\n"]}],"source":["model = sbert_GRU(input_dim, hidden_dim, linear_dim, output_dim).to(device)\n","# model.init_weights()\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"c69ad5bd","metadata":{"id":"c69ad5bd","outputId":"df405d59-1831-41f9-9cca-a8ea1cce56be"},"outputs":[{"data":{"text/plain":["array([0.6468254 , 3.74459459])"]},"execution_count":440,"metadata":{},"output_type":"execute_result"}],"source":["class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n","class_weights[1] = 1.7 * class_weights[1]\n","class_weights"]},{"cell_type":"code","execution_count":null,"id":"c535f55f","metadata":{"id":"c535f55f"},"outputs":[],"source":["class_weights = torch.FloatTensor(class_weights)\n","loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"]},{"cell_type":"code","execution_count":null,"id":"b2154d24","metadata":{"id":"b2154d24","outputId":"48647a50-470e-4700-a39c-1006521ae383"},"outputs":[{"data":{"text/plain":["2.546875"]},"execution_count":442,"metadata":{},"output_type":"execute_result"}],"source":["len(sbert_train_dataset)/batch_size"]},{"cell_type":"code","execution_count":null,"id":"5938432d","metadata":{"id":"5938432d","outputId":"e00d571f-540a-4cbc-e098-85f9098ea176"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\r"]},{"ename":"NameError","evalue":"name 'accuracy' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20316/457713654.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msbert_val_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"]}],"source":["epochs = 30\n","dev_max_acc = 0\n","for epoch in range(epochs):\n","    model.train()\n","    train_acc = 0.0\n","    dev_acc = 0.0\n","    batch = 0\n","    count = 0\n","    for inputs, target, lens in sbert_train_loader:\n","        \n","        print(batch, end = \"\\r\")\n","        batch += 1\n","        inputs, target = inputs.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(inputs, lens)\n","        output = output.view(-1, output.shape[-1])\n","        target = target.view(-1)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_acc += float(accuracy(output, target).item())\n","    model.eval()\n","    for inputs, target, lens in sbert_val_loader:\n","        inputs, target = inputs.to(device), target.to(device)\n","        inputs, target = inputs, target\n","        output = model(inputs, lens)\n","        output = output.view(-1, output.shape[-1])\n","        target = target.view(-1)\n","        loss = loss_fn(output, target)\n","        dev_acc += float(accuracy(output, target).item())\n","    \n","    train_acc = train_acc / ((len(sbert_train_loader.dataset) / batch_size) + 1)\n","    dev_acc = dev_acc / ((len(sbert_val_loader.dataset) / batch_size) + 1)\n","    \n","    print('Epoch: {} \\tTraining Acc: {:.6f} \\tDev Set Acc: {:.6f}'.format(epoch+1, train_acc,dev_acc))\n","    if dev_acc >= dev_max_acc:\n","        print('Dev Set acc increased ({:.6f} --> {:.6f}). Saving model...'.format(dev_max_acc, dev_acc))\n","        torch.save(model.state_dict(), 'model/sbert_GRU.pt')\n","        dev_max_acc = dev_acc "]},{"cell_type":"code","execution_count":null,"id":"631c1588","metadata":{"id":"631c1588"},"outputs":[],"source":["del model, loss_fn, optimizer"]},{"cell_type":"code","execution_count":null,"id":"5de9bf71","metadata":{"id":"5de9bf71","outputId":"824985c4-dcbb-4d9c-a22f-65962b32d9f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1:  0.6588628762541806\n","Accuracy:  0.6851851851851852\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["model = sbert_GRU(input_dim, hidden_dim, linear_dim, output_dim).to(device)\n","model.load_state_dict(torch.load(\"model/sbert_GRU.pt\"))\n","\n","model.eval()\n","targets = None\n","preds = None\n","with torch.no_grad():\n","    for inputs, target, lens in sbert_test_loader:\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = model(inputs, lens)\n","        output = output.view(-1, output.shape[-1])\n","        target = target.view(-1)\n","        _, pred = torch.max(output, 1)\n","#         print(target.shape, pred.shape)\n","        if targets == None:\n","            targets = target\n","            preds = pred\n","        else:\n","            targets = torch.cat((targets, target), 0)\n","            preds = torch.cat((preds, pred), 0)\n","targets, preds = targets.cpu().detach().numpy(), preds.cpu().detach().numpy()\n","print(\"F1: \", f1_score(targets, preds, average = \"macro\"))\n","print(\"Accuracy: \", accuracy_score(targets, preds))\n","# print(\"CCC: \", concordance_correlation_coefficient(targets, preds))\n","cm = confusion_matrix(targets, preds)\n","cmd_obj = ConfusionMatrixDisplay(cm, display_labels=np.unique(train_labels))\n","cmd_obj.plot()\n","cmd_obj.ax_.set(title='Confusion Matrix', xlabel='Predicted Labels', ylabel='True Labels')\n","plt.show()\n","\n","del model"]},{"cell_type":"code","execution_count":null,"id":"bbc05153","metadata":{"id":"bbc05153"},"outputs":[],"source":["class PadSequence_regression_bert:\n","    def __call__(self, batch):\n","        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n","        sequences = [torch.FloatTensor(x[0]) for x in sorted_batch]\n","        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n","        lengths = torch.FloatTensor([len(x) for x in sequences])\n","        scores = torch.FloatTensor([x[1] for x in sorted_batch])\n","        return torch.FloatTensor(sequences_padded), torch.FloatTensor(scores), lengths"]},{"cell_type":"code","execution_count":null,"id":"e21dfe0f","metadata":{"id":"e21dfe0f"},"outputs":[],"source":["sbert_train_reg = list(zip(sbert_train, train_score))\n","sbert_val_reg = list(zip(sbert_val, val_score))\n","sbert_test_reg = list(zip(sbert_test, test_score))"]},{"cell_type":"code","execution_count":null,"id":"261b34a0","metadata":{"id":"261b34a0"},"outputs":[],"source":["batch_size = 4"]},{"cell_type":"code","execution_count":null,"id":"b1dd3575","metadata":{"id":"b1dd3575"},"outputs":[],"source":["sbert_train_loader_reg = DataLoader(sbert_train_reg, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression_bert())\n","sbert_val_loader_reg = DataLoader(sbert_val_reg, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression_bert())\n","sbert_test_loader_reg = DataLoader(sbert_test_reg, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression_bert())"]},{"cell_type":"code","execution_count":null,"id":"f2c3ab2a","metadata":{"id":"f2c3ab2a"},"outputs":[],"source":["class sbert_GRU_regression(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, linear_dim_1, linear_dim_2):\n","        super(sbert_GRU_regression, self).__init__()\n","        self.pad_idx = pad_idx\n","#         self.linear_dim = linear_dim\n","        self.gru = torch.nn.GRU(input_size = input_dim, hidden_size = hidden_dim, num_layers = 1, bidirectional = False, batch_first = True, dropout = 0)\n","        self.dropout = torch.nn.Dropout(p = 0)\n","        self.linear_1 = torch.nn.Linear(hidden_dim, linear_dim_1)\n","#         self.linear_2 = torch.nn.Linear(linear_dim_1, linear_dim_2)\n","#         self.linear_3 = torch.nn.Linear(linear_dim_2, linear_dim_3)\n","#         self.relu = torch.nn.ReLU()\n","        self.output = torch.nn.Linear(linear_dim_1, 1)\n","    \n","    def forward(self, x, lengths):\n","        packed = pack_padded_sequence(x, lengths, batch_first = True, enforce_sorted = True)\n","        gru_out, h_t = self.gru(packed)\n","#         print(h_t.shape)\n","#         print(h_t[-1, :, :].shape)\n","#         gru_out = self.dropout(gru_out[:, -1])\n","#         gru_out = gru_out[:, -1, :]\n","#         h_t = h_t[-1, :, :]\n","        lin_out_1 = self.dropout(self.linear_1(self.dropout(h_t[-1])))\n","#         lin_out_2 = self.dropout(self.linear_2(lin_out_1))\n","#         lin_out_3 = self.dropout(self.linear_3(lin_out_2))\n","        out = self.output(lin_out_1)\n","        return out"]},{"cell_type":"code","execution_count":null,"id":"9f6f31ad","metadata":{"id":"9f6f31ad"},"outputs":[],"source":["input_dim = 768\n","hidden_dim = 1024\n","linear_dim_1 = 512\n","linear_dim_2 = linear_dim // 2\n","\n","model = sbert_GRU_regression(input_dim, hidden_dim, linear_dim_1, linear_dim_2).to(device)\n","# model.init_weights()\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"d574c627","metadata":{"id":"d574c627"},"outputs":[],"source":["loss_fn = torch.nn.MSELoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-6)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2, gamma=0.9, last_epoch=- 1, verbose=True)"]},{"cell_type":"code","execution_count":null,"id":"f276e8a9","metadata":{"id":"f276e8a9"},"outputs":[],"source":["epochs = 20\n","dev_min_loss = np.inf\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    dev_loss = 0.0\n","    batch = 0\n","    for inputs, target, lens in sbert_train_loader_reg:\n","        print(batch, end = \"\\r\")\n","        batch += 1\n","        inputs, target = inputs.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(inputs, lens)\n","        output = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        loss = torch.sqrt(loss_fn(output, target))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","    model.eval()\n","    for inputs, target, lens in sbert_val_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        inputs, target = inputs, target\n","        output = model(inputs, lens)\n","        output = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        loss = torch.sqrt(loss_fn(output, target))\n","        dev_loss += loss.item()\n","    \n","    train_loss = train_loss / len(sbert_train_loader_reg)\n","    dev_loss = dev_loss / len(sbert_val_loader_reg)\n","#     scheduler.step()\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tDev Set Loss: {:.6f}'.format(epoch+1, train_loss,dev_loss))\n","    if dev_loss <= dev_min_loss:\n","        print('Dev Set Loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(dev_min_loss, dev_loss))\n","        torch.save(model.state_dict(), 'model/sbert_GRU_reg.pt')\n","        dev_min_loss = dev_loss "]},{"cell_type":"code","execution_count":null,"id":"ae6bfdb7","metadata":{"id":"ae6bfdb7"},"outputs":[],"source":["del model, loss_fn, optimizer"]},{"cell_type":"code","execution_count":null,"id":"8e932f7f","metadata":{"id":"8e932f7f","outputId":"9d68cefd-f3c6-4d55-dac2-1592453006e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target: [[19.]\n"," [15.]\n"," [ 3.]\n"," [13.]\n"," [ 0.]\n"," [ 4.]\n"," [16.]\n"," [ 0.]\n"," [ 0.]\n"," [ 7.]\n"," [ 7.]\n"," [ 5.]\n"," [15.]\n"," [ 1.]\n"," [16.]\n"," [ 7.]\n"," [19.]\n"," [ 5.]\n"," [ 5.]\n"," [ 3.]\n"," [ 7.]\n"," [11.]\n"," [11.]\n"," [ 9.]\n"," [ 2.]\n"," [ 0.]\n"," [17.]\n"," [ 0.]\n"," [ 9.]\n"," [ 0.]\n"," [12.]\n"," [ 6.]\n"," [13.]\n"," [22.]\n"," [20.]\n"," [ 8.]\n"," [13.]\n"," [ 2.]\n"," [ 5.]\n"," [ 2.]\n"," [ 3.]\n"," [10.]\n"," [ 0.]\n"," [ 2.]\n"," [16.]\n"," [ 3.]\n"," [ 0.]\n"," [19.]\n"," [ 8.]\n"," [12.]\n"," [ 9.]\n"," [ 0.]\n"," [19.]\n"," [12.]]\n","Preds: [[14.124115 ]\n"," [15.636131 ]\n"," [ 4.217386 ]\n"," [ 5.995835 ]\n"," [ 8.62693  ]\n"," [ 4.446741 ]\n"," [ 7.5090814]\n"," [ 4.021942 ]\n"," [ 6.510285 ]\n"," [12.22932  ]\n"," [ 6.262854 ]\n"," [ 7.5575414]\n"," [16.904139 ]\n"," [ 4.6930003]\n"," [10.00125  ]\n"," [ 7.061578 ]\n"," [14.868844 ]\n"," [ 6.4085536]\n"," [ 9.062409 ]\n"," [ 2.523838 ]\n"," [ 3.5907586]\n"," [ 9.590245 ]\n"," [15.1033   ]\n"," [ 6.0128093]\n"," [ 5.6329784]\n"," [ 4.2733603]\n"," [ 4.602468 ]\n"," [ 6.484957 ]\n"," [ 7.7310977]\n"," [ 6.7098894]\n"," [11.292905 ]\n"," [ 2.2600782]\n"," [12.660085 ]\n"," [13.822804 ]\n"," [11.573426 ]\n"," [ 4.449501 ]\n"," [ 5.1349435]\n"," [ 1.5263994]\n"," [ 2.6154904]\n"," [ 2.6298807]\n"," [ 5.2742133]\n"," [ 6.294449 ]\n"," [ 4.470383 ]\n"," [ 3.9025707]\n"," [14.002236 ]\n"," [ 8.63624  ]\n"," [ 4.4830017]\n"," [ 3.2273898]\n"," [ 7.2132483]\n"," [ 9.089576 ]\n"," [ 2.961785 ]\n"," [ 2.806407 ]\n"," [20.775446 ]\n"," [ 7.2897677]]\n","RMSE Score:  5.0476055\n","CCC:  0.5827077995159888\n","\n"]}],"source":["model = sbert_GRU_regression(input_dim, hidden_dim, linear_dim_1, linear_dim_2).to(device)\n","model.load_state_dict(torch.load(\"model/sbert_GRU_reg.pt\"))\n","model.eval()\n","if batch_size == 1:\n","    targets = []\n","    preds = []\n","else:\n","    targets = None\n","with torch.no_grad():\n","    for inputs, target, lens in sbert_test_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = model(inputs, lens)\n","        pred = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        if batch_size == 1:\n","            targets.append(target.item())\n","            preds.append(pred.item())\n","        else:\n","            if targets == None:\n","                targets = target\n","                preds = pred\n","            else:\n","                targets = torch.cat((targets, target), 0)\n","                preds = torch.cat((preds, pred), 0)\n","if batch_size == 1:\n","    targets = np.expand_dims(np.array(targets), axis = 1)\n","    preds = np.expand_dims(np.array(preds), axis = 1)\n","else:\n","    targets, preds = targets.cpu().detach().numpy(), preds.cpu().detach().numpy()\n","    # targets = scaler.inverse_transform(np.expand_dims(targets, axis = 1))\n","    # preds = scaler.inverse_transform(np.expand_dims(preds, axis = 1))\n","    targets = np.expand_dims(targets, axis = 1)\n","    preds = np.expand_dims(preds, axis = 1)\n","print(\"Target:\", targets)\n","print(\"Preds:\", preds)\n","print(\"RMSE Score: \", mean_squared_error(targets, preds, squared = False))\n","targets, preds = np.squeeze(targets), np.squeeze(preds)\n","# print(\"Accuracy: \", accuracy_score(targets, preds))\n","print(\"CCC: \", concordance_correlation_coefficient(targets, preds))\n","# cm = confusion_matrix(targets, preds)\n","# cmd_obj = ConfusionMatrixDisplay(cm, display_labels=np.unique(train_labels))\n","# cmd_obj.plot()\n","# cmd_obj.ax_.set(title='Confusion Matrix', xlabel='Predicted Labels', ylabel='True Labels')\n","# plt.show()\n","# print(\"\\n\", targets)\n","# print(preds)\n","print()\n","del model"]},{"cell_type":"code","execution_count":null,"id":"d83f5694","metadata":{"id":"d83f5694"},"outputs":[],"source":["train_labels = pd.read_csv(\"Fusion/train_split.csv\")\n","dev_labels = pd.read_csv(\"Fusion/dev_split.csv\")"]},{"cell_type":"code","execution_count":null,"id":"8d7a1136","metadata":{"id":"8d7a1136"},"outputs":[],"source":["train_labels.drop(columns=[\"Gender\", \"PCL-C (PTSD)\", \"PTSD Severity\"], inplace=True)\n","dev_labels.drop(columns=[\"Gender\", \"PCL-C (PTSD)\", \"PTSD Severity\"], inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"0f4989b0","metadata":{"id":"0f4989b0"},"outputs":[],"source":["train_labels"]},{"cell_type":"code","execution_count":null,"id":"5d13d176","metadata":{"id":"5d13d176"},"outputs":[],"source":["egemaps_data = {}\n","ege_df = pd.read_pickle(\"Fusion\\egemaps_df_padd.pkl\")"]},{"cell_type":"code","execution_count":null,"id":"b09a69fc","metadata":{"id":"b09a69fc"},"outputs":[],"source":["ege_df.drop(columns=['Unnamed: 0', 'name', 'frameTime'], inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"a1ef8984","metadata":{"id":"a1ef8984"},"outputs":[],"source":["ege_df[\"patient_id\"].unique()"]},{"cell_type":"code","execution_count":null,"id":"fa44a8dc","metadata":{"id":"fa44a8dc"},"outputs":[],"source":["for i in range(len)"]},{"cell_type":"code","execution_count":null,"id":"26ad4897","metadata":{"id":"26ad4897"},"outputs":[],"source":["SEQ_LENGTH=20*60*100"]},{"cell_type":"code","execution_count":null,"id":"82ce01d7","metadata":{"id":"82ce01d7"},"outputs":[],"source":["ege_train = {}\n","ege_dev = {}\n","train_idx = 0\n","dev_idx = 0\n","for idx in egemaps_data:\n","    print(idx)\n","    if egemaps_data[idx][\"id\"] in train_labels[\"Participant_ID\"].values:\n","        ege_train[train_idx] = egemaps_data[idx]\n","        train_idx += 1\n","    else:\n","        ege_dev[dev_idx] = egemaps_data[idx]\n","        dev_idx += 1"]},{"cell_type":"code","execution_count":null,"id":"f3f3881c","metadata":{"id":"f3f3881c","outputId":"55a41f91-3f2f-491a-b548-831bf52a9e13"},"outputs":[{"name":"stdout","output_type":"stream","text":["26160000\r"]}],"source":["ege_train = {}\n","ege_dev = {}\n","\n","for i in range(0, len(ege_df), 120000):\n","    print(i, end = \"\\r\")\n","    pid = ege_df.iloc[i][\"patient_id\"]\n","    if pid in train_labels[\"Participant_ID\"].values:\n","        ege_train[pid] = ege_df.iloc[i:i+120000].drop(['Unnamed: 0', 'name', 'frameTime', 'patient_id', 'bin','req'], axis = 1).values\n","    else:\n","        ege_dev[pid] = ege_df.iloc[i:i+120000].drop(['Unnamed: 0', 'name', 'frameTime', 'patient_id', 'bin','req'], axis = 1).values"]},{"cell_type":"code","execution_count":null,"id":"304f23e1","metadata":{"id":"304f23e1"},"outputs":[],"source":["ege_train = dict(sorted(ege_train.items()))\n","ege_dev = dict(sorted(ege_dev.items()))"]},{"cell_type":"code","execution_count":null,"id":"35056bee","metadata":{"id":"35056bee"},"outputs":[],"source":["ege_train_data = []\n","ege_dev_data = []\n","for key in ege_train:\n","    ege_train_data.append(ege_train[key])\n","for key in ege_dev:\n","    ege_dev_data.append(ege_dev[key])\n","ege_train_data = np.array(ege_train_data)\n","ege_dev_data = np.array(ege_dev_data)"]},{"cell_type":"code","execution_count":null,"id":"977d14dd","metadata":{"id":"977d14dd","outputId":"26c3ee7c-2cec-4fe9-9ad2-539ad8dab3d5"},"outputs":[{"data":{"text/plain":["(56, 120000, 23)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["ege_dev_data.shape"]},{"cell_type":"code","execution_count":null,"id":"36ec687d","metadata":{"id":"36ec687d"},"outputs":[],"source":["import pickle as pkl\n","with open(\"ege_train.pkl\", \"wb\") as fp:\n","    pkl.dump(ege_train_data, fp)\n","    \n","with open(\"ege_dev.pkl\", \"wb\") as fp:\n","    pkl.dump(ege_dev_data, fp)"]},{"cell_type":"code","execution_count":null,"id":"c0d05628","metadata":{"id":"c0d05628"},"outputs":[],"source":["from torch.nn.functional import interpolate as interpolate\n","def create_fusion_data(sbert_data, ege_data):\n","    fusion_data = []\n","    ege_down = []\n","    for sbert, ege in zip(sbert_data, ege_data):\n","        sbert = torch.FloatTensor(sbert)\n","        ege = torch.FloatTensor(ege)\n","        ratio = len(sbert)\n","        sbert_t = sbert.T[None, :]\n","        ege_t = ege.T[None, :]\n","        ege_t = interpolate(ege_t, size = [len(sbert)])\n","        sbert = sbert_t[0, :].T\n","        ege = ege_t[0, :].T\n","        fusion = torch.cat((sbert, ege), dim = 1).cpu().detach().numpy()\n","        fusion_data.append(fusion)\n","        ege_down.append(ege)\n","    return ege_down, sbert, fusion_data"]},{"cell_type":"code","execution_count":null,"id":"597675fa","metadata":{"id":"597675fa"},"outputs":[],"source":["ege_train, _, fusion_train = create_fusion_data(sbert_train, ege_train_data)"]},{"cell_type":"code","execution_count":null,"id":"d6b12521","metadata":{"id":"d6b12521"},"outputs":[],"source":["ege_val, _, fusion_val = create_fusion_data(sbert_val, ege_dev_data)"]},{"cell_type":"code","execution_count":null,"id":"b6458310","metadata":{"id":"b6458310"},"outputs":[],"source":["ege_train_dataset = list(zip(ege_train, train_score))\n","ege_val_dataset = list(zip(ege_val, val_score))"]},{"cell_type":"code","execution_count":null,"id":"d4f9a077","metadata":{"id":"d4f9a077"},"outputs":[],"source":["class PadSequence_regression_bert:\n","    def __call__(self, batch):\n","        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n","        sequences = [torch.FloatTensor(x[0]) for x in sorted_batch]\n","        sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n","        lengths = torch.FloatTensor([len(x) for x in sequences])\n","        scores = torch.FloatTensor([x[1] for x in sorted_batch])\n","        return torch.FloatTensor(sequences_padded), torch.FloatTensor(scores), lengths"]},{"cell_type":"code","execution_count":null,"id":"f59cf2d6","metadata":{"id":"f59cf2d6"},"outputs":[],"source":["batch_size = 2"]},{"cell_type":"code","execution_count":null,"id":"801ba58a","metadata":{"id":"801ba58a"},"outputs":[],"source":["ege_train_loader_reg = DataLoader(ege_train_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression_bert())\n","ege_val_loader_reg = DataLoader(ege_val_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression_bert())"]},{"cell_type":"code","execution_count":null,"id":"b0e3fa97","metadata":{"id":"b0e3fa97"},"outputs":[],"source":["train_on_gpu=torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"id":"277c49db","metadata":{"id":"277c49db"},"outputs":[],"source":["class ege_GRU_regression(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, linear_dim, linear_dim_2, n_layers, output_size, drop_prob):\n","        super(ege_GRU_regression, self).__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.linear_dim = linear_dim\n","        self.linear_dim_2 = linear_dim_2\n","        self.drop_prob=drop_prob\n","        self.dropout = torch.nn.Dropout(p=self.drop_prob)\n","\n","        self.output_size = output_size\n","        \n","        \n","        self.gru = torch.nn.GRU(self.input_dim, self.hidden_dim, self.n_layers, batch_first=True)\n","        \n","\n","        self.linear = torch.nn.Linear(self.hidden_dim, self.linear_dim)\n","        self.linear_2 = torch.nn.Linear(self.linear_dim, self.linear_dim_2)\n","        self.fc = torch.nn.Linear(self.linear_dim_2, self.output_size)\n","#         self.RELU = torch.nn.ReLU()\n","    \n","    def forward(self, x, lengths):\n","        #x = x.long()\n","        packed = pack_padded_sequence(x, lengths, batch_first = True, enforce_sorted = True)\n","        gru_out, h_t = self.gru(packed)\n","        #print(lstm_out)\n","        # getting the last time step output\n","        \n","        h_out = h_t[-1]\n","        linear = self.dropout(self.linear(h_out))\n","        linear_2 = self.dropout(self.linear_2(linear))\n","        #print(\"LSTM_out\",out)\n","        out = self.fc(linear_2)\n","        #print(out)\n","#         out = self.RELU(out)\n","        #out = self.sig(out)\n","        \n","        return out"]},{"cell_type":"code","execution_count":null,"id":"6abc5af4","metadata":{"id":"6abc5af4"},"outputs":[],"source":["input_dim = 23\n","hidden_dim = 256\n","linear_dim = 128\n","linear_dim_2 = linear_dim // 2\n","n_layers = 2\n","output_size = 1\n","drop_prob=0.2"]},{"cell_type":"code","execution_count":null,"id":"4342ae60","metadata":{"id":"4342ae60","outputId":"195999df-21a9-411a-c8cd-4cb82487acd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["ege_GRU_regression(\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (gru): GRU(23, 256, num_layers=2, batch_first=True)\n","  (linear): Linear(in_features=256, out_features=128, bias=True)\n","  (linear_2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc): Linear(in_features=64, out_features=1, bias=True)\n",")\n"]}],"source":["model = ege_GRU_regression(input_dim, hidden_dim, linear_dim, linear_dim_2, n_layers, output_size, drop_prob).to(device)\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"db4f22d9","metadata":{"id":"db4f22d9"},"outputs":[],"source":["loss_fn = torch.nn.MSELoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-6)"]},{"cell_type":"code","execution_count":null,"id":"e9b1db06","metadata":{"id":"e9b1db06","outputId":"b5415717-d7bf-4cc4-b703-654921089b76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 \tTraining Loss: 6.286361 \tDev Set Loss: 5.206619\n","Dev Set Loss decreased (inf --> 5.206619). Saving model...\n","14\r"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\nsuka\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 \tTraining Loss: 5.549934 \tDev Set Loss: 5.198922\n","Dev Set Loss decreased (5.206619 --> 5.198922). Saving model...\n","Epoch: 3 \tTraining Loss: 5.328710 \tDev Set Loss: 5.081283\n","Dev Set Loss decreased (5.198922 --> 5.081283). Saving model...\n","Epoch: 4 \tTraining Loss: 5.496516 \tDev Set Loss: 5.099389\n","Epoch: 5 \tTraining Loss: 5.542003 \tDev Set Loss: 5.199020\n","Epoch: 6 \tTraining Loss: 5.512590 \tDev Set Loss: 5.118595\n","Epoch: 7 \tTraining Loss: 5.526781 \tDev Set Loss: 5.099550\n","Epoch: 8 \tTraining Loss: 5.402089 \tDev Set Loss: 5.091760\n","Epoch: 9 \tTraining Loss: 5.435784 \tDev Set Loss: 5.137089\n","Epoch: 10 \tTraining Loss: 5.403621 \tDev Set Loss: 5.093310\n","Epoch: 11 \tTraining Loss: 5.494804 \tDev Set Loss: 5.119507\n","Epoch: 12 \tTraining Loss: 5.430522 \tDev Set Loss: 5.089976\n","Epoch: 13 \tTraining Loss: 5.332207 \tDev Set Loss: 4.925551\n","Dev Set Loss decreased (5.081283 --> 4.925551). Saving model...\n","Epoch: 14 \tTraining Loss: 5.389719 \tDev Set Loss: 5.009062\n","Epoch: 15 \tTraining Loss: 5.468251 \tDev Set Loss: 5.047411\n","Epoch: 16 \tTraining Loss: 5.475627 \tDev Set Loss: 4.971883\n","Epoch: 17 \tTraining Loss: 5.488052 \tDev Set Loss: 5.033914\n","Epoch: 18 \tTraining Loss: 5.185724 \tDev Set Loss: 5.076879\n","Epoch: 19 \tTraining Loss: 5.257970 \tDev Set Loss: 5.084759\n","Epoch: 20 \tTraining Loss: 5.208523 \tDev Set Loss: 4.870868\n","Dev Set Loss decreased (4.925551 --> 4.870868). Saving model...\n"]}],"source":["epochs = 20\n","dev_min_loss = np.inf\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    dev_loss = 0.0\n","    batch = 0\n","    for inputs, target, lens in ege_train_loader_reg:\n","        print(batch, end = \"\\r\")\n","        batch += 1\n","        inputs, target = inputs.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(inputs, lens)\n","        output = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        loss = torch.sqrt(loss_fn(output, target))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","    model.eval()\n","    for inputs, target, lens in ege_val_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        inputs, target = inputs, target\n","        with torch.no_grad():\n","            output = model(inputs, lens)\n","            output = torch.squeeze(output.view(-1, output.shape[-1]))\n","            target = target.view(-1)\n","            loss = torch.sqrt(loss_fn(output, target))\n","            dev_loss += loss.item()\n","    \n","    train_loss = train_loss / len(ege_train_loader_reg)\n","    dev_loss = dev_loss / len(ege_val_loader_reg)\n","#     scheduler.step()\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tDev Set Loss: {:.6f}'.format(epoch+1, train_loss,dev_loss))\n","    if dev_loss <= dev_min_loss:\n","        print('Dev Set Loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(dev_min_loss, dev_loss))\n","        torch.save(model.state_dict(), 'model/ege_reg.pt')\n","        dev_min_loss = dev_loss "]},{"cell_type":"code","execution_count":null,"id":"e8cc9e1f","metadata":{"id":"e8cc9e1f"},"outputs":[],"source":["del model, loss_fn, optimizer"]},{"cell_type":"code","execution_count":null,"id":"db0904b4","metadata":{"id":"db0904b4","outputId":"10a9a3b5-0046-4451-8e1f-afb94784a231"},"outputs":[{"name":"stdout","output_type":"stream","text":["Target: [[19.]\n"," [ 9.]\n"," [ 2.]\n"," [ 9.]\n"," [ 3.]\n"," [ 2.]\n"," [ 1.]\n"," [ 2.]\n"," [ 1.]\n"," [ 0.]\n"," [18.]\n"," [ 1.]\n"," [ 2.]\n"," [ 1.]\n"," [ 7.]\n"," [12.]\n"," [12.]\n"," [ 9.]\n"," [ 4.]\n"," [ 7.]\n"," [ 8.]\n"," [ 4.]\n"," [ 4.]\n"," [16.]\n"," [ 2.]\n"," [ 0.]\n"," [ 7.]\n"," [ 3.]\n"," [ 8.]\n"," [ 6.]\n"," [ 6.]\n"," [11.]\n"," [16.]\n"," [ 0.]\n"," [ 0.]\n"," [20.]\n"," [11.]\n"," [ 4.]\n"," [12.]\n"," [ 3.]\n"," [ 0.]\n"," [ 5.]\n"," [ 0.]\n"," [ 6.]\n"," [11.]\n"," [ 0.]\n"," [ 9.]\n"," [18.]\n"," [10.]\n"," [ 0.]\n"," [17.]\n"," [ 9.]\n"," [11.]\n"," [ 0.]\n"," [ 8.]\n"," [ 3.]]\n","Preds: [[9.979127 ]\n"," [5.9197745]\n"," [7.273878 ]\n"," [5.1258926]\n"," [8.500287 ]\n"," [4.5598087]\n"," [6.2919984]\n"," [5.7912164]\n"," [7.950666 ]\n"," [3.1964128]\n"," [7.638751 ]\n"," [5.4650326]\n"," [6.443321 ]\n"," [3.105336 ]\n"," [5.2980547]\n"," [6.7097836]\n"," [6.9980145]\n"," [7.47969  ]\n"," [6.7329016]\n"," [6.004429 ]\n"," [7.9799986]\n"," [7.5583787]\n"," [4.6545095]\n"," [3.5475037]\n"," [7.5300593]\n"," [4.493306 ]\n"," [8.59132  ]\n"," [5.732936 ]\n"," [6.7443504]\n"," [5.685242 ]\n"," [6.9715986]\n"," [7.3769097]\n"," [6.3889947]\n"," [6.3157187]\n"," [7.036287 ]\n"," [7.5356855]\n"," [5.2425756]\n"," [3.731123 ]\n"," [4.8202615]\n"," [4.806421 ]\n"," [3.3298512]\n"," [6.817936 ]\n"," [6.5268106]\n"," [3.7535665]\n"," [3.3315947]\n"," [3.5197062]\n"," [8.308314 ]\n"," [6.905096 ]\n"," [6.403313 ]\n"," [4.886452 ]\n"," [6.6603274]\n"," [2.7837272]\n"," [7.4247804]\n"," [6.3228183]\n"," [5.4044094]\n"," [5.342338 ]]\n","RMSE Score:  5.450313\n","CCC:  0.15015449159820934\n","\n"]}],"source":["model = ege_GRU_regression(input_dim, hidden_dim, linear_dim, linear_dim_2, n_layers, output_size, drop_prob).to(device)\n","model.load_state_dict(torch.load(\"model/ege_reg.pt\"))\n","model.eval()\n","if batch_size == 1:\n","    targets = []\n","    preds = []\n","else:\n","    targets = None\n","with torch.no_grad():\n","    for inputs, target, lens in ege_val_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = model(inputs, lens)\n","        pred = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        if batch_size == 1:\n","            targets.append(target.item())\n","            preds.append(pred.item())\n","        else:\n","            if targets == None:\n","                targets = target\n","                preds = pred\n","            else:\n","                targets = torch.cat((targets, target), 0)\n","                preds = torch.cat((preds, pred), 0)\n","if batch_size == 1:\n","    targets = np.expand_dims(np.array(targets), axis = 1)\n","    preds = np.expand_dims(np.array(preds), axis = 1)\n","else:\n","    targets, preds = targets.cpu().detach().numpy(), preds.cpu().detach().numpy()\n","    # targets = scaler.inverse_transform(np.expand_dims(targets, axis = 1))\n","    # preds = scaler.inverse_transform(np.expand_dims(preds, axis = 1))\n","    targets = np.expand_dims(targets, axis = 1)\n","    preds = np.expand_dims(preds, axis = 1)\n","print(\"Target:\", targets)\n","print(\"Preds:\", preds)\n","print(\"RMSE Score: \", mean_squared_error(targets, preds, squared = False))\n","targets, preds = np.squeeze(targets), np.squeeze(preds)\n","# print(\"Accuracy: \", accuracy_score(targets, preds))\n","print(\"CCC: \", concordance_correlation_coefficient(targets, preds))\n","# cm = confusion_matrix(targets, preds)\n","# cmd_obj = ConfusionMatrixDisplay(cm, display_labels=np.unique(train_labels))\n","# cmd_obj.plot()\n","# cmd_obj.ax_.set(title='Confusion Matrix', xlabel='Predicted Labels', ylabel='True Labels')\n","# plt.show()\n","# print(\"\\n\", targets)\n","# print(preds)\n","print()\n","del model"]},{"cell_type":"code","execution_count":null,"id":"d9038d8a","metadata":{"id":"d9038d8a"},"outputs":[],"source":["audio_ccc = 0.15\n","text_ccc = 0.583\n","total = audio_ccc + text_ccc\n","audio_ratio = audio_ccc / total\n","text_ratio = text_ccc / total"]},{"cell_type":"code","execution_count":null,"id":"04283f79","metadata":{"id":"04283f79","outputId":"3900b02d-c8b0-4ce6-d92c-d892f51ea152"},"outputs":[{"data":{"text/plain":["0.20463847203274216"]},"execution_count":487,"metadata":{},"output_type":"execute_result"}],"source":["audio_ratio"]},{"cell_type":"code","execution_count":null,"id":"00b0ba22","metadata":{"id":"00b0ba22","outputId":"3da073d4-7772-4014-c3e2-e9ba98e33799"},"outputs":[{"data":{"text/plain":["0.7953615279672578"]},"execution_count":488,"metadata":{},"output_type":"execute_result"}],"source":["text_ratio"]},{"cell_type":"code","execution_count":null,"id":"b8eb0a33","metadata":{"id":"b8eb0a33"},"outputs":[],"source":["input_dim = 23\n","hidden_dim = 256\n","linear_dim = 128\n","linear_dim_2 = linear_dim // 2\n","n_layers = 2\n","output_size = 1\n","drop_prob=0.2\n","audio_model = ege_GRU_regression(input_dim, hidden_dim, linear_dim, linear_dim_2, n_layers, output_size, drop_prob).to(device)\n","audio_model.load_state_dict(torch.load(\"model/ege_reg.pt\"))\n","audio_model.eval()\n","\n","input_dim = 768\n","hidden_dim = 1024\n","linear_dim_1 = 512\n","linear_dim_2 = linear_dim // 2\n","text_model = sbert_GRU_regression(input_dim, hidden_dim, linear_dim_1, linear_dim_2).to(device)\n","text_model.load_state_dict(torch.load(\"model/sbert_GRU_reg.pt\"))\n","text_model.eval()\n","\n","targets = None\n","with torch.no_grad():\n","    for inputs, target, lens in ege_val_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = audio_model(inputs, lens)\n","        pred = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        if targets == None:\n","            targets = target\n","            preds = pred\n","        else:\n","            targets = torch.cat((targets, target), 0)\n","            preds = torch.cat((preds, pred), 0)\n","targets, preds = targets.cpu().detach().numpy(), preds.cpu().detach().numpy()\n","# targets = scaler.inverse_transform(np.expand_dims(targets, axis = 1))\n","# preds = scaler.inverse_transform(np.expand_dims(preds, axis = 1))\n","targets = np.expand_dims(targets, axis = 1)\n","preds = np.expand_dims(preds, axis = 1)\n","audio_preds = preds \n","\n","targets = None\n","with torch.no_grad():\n","    for inputs, target, lens in sbert_val_loader_reg:\n","        inputs, target = inputs.to(device), target.to(device)\n","        output = text_model(inputs, lens)\n","        pred = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        if targets == None:\n","            targets = target\n","            preds = pred\n","        else:\n","            targets = torch.cat((targets, target), 0)\n","            preds = torch.cat((preds, pred), 0)\n","targets, preds = targets.cpu().detach().numpy(), preds.cpu().detach().numpy()\n","# targets = scaler.inverse_transform(np.expand_dims(targets, axis = 1))\n","# preds = scaler.inverse_transform(np.expand_dims(preds, axis = 1))\n","targets = np.expand_dims(targets, axis = 1)\n","preds = np.expand_dims(preds, axis = 1)\n","text_preds = preds \n","del audio_model, text_model"]},{"cell_type":"code","execution_count":null,"id":"3b9874fd","metadata":{"id":"3b9874fd"},"outputs":[],"source":["fusion_preds = audio_preds + text_preds"]},{"cell_type":"code","execution_count":null,"id":"f8601290","metadata":{"id":"f8601290","outputId":"c7f04e4b-f281-4f79-e2e1-7ec2c2f9c780"},"outputs":[{"name":"stdout","output_type":"stream","text":["RMSE Score:  7.8887587\n","CCC:  0.26125197515339016\n"]}],"source":["print(\"RMSE Score: \", mean_squared_error(targets, fusion_preds, squared = False))\n","targets, fusion_preds = np.squeeze(targets), np.squeeze(fusion_preds)\n","# print(\"Accuracy: \", accuracy_score(targets, preds))\n","print(\"CCC: \", concordance_correlation_coefficient(targets, fusion_preds))"]},{"cell_type":"code","execution_count":null,"id":"6effab70","metadata":{"id":"6effab70"},"outputs":[],"source":["fusion_train_dataset = list(zip(fusion_train, train_score))\n","fusion_val_dataset = list(zip(fusion_val, val_score))"]},{"cell_type":"code","execution_count":null,"id":"4e43c84d","metadata":{"id":"4e43c84d","outputId":"05afbe68-5bde-4216-e792-3a202488ccc1"},"outputs":[{"data":{"text/plain":["(38, 791)"]},"execution_count":501,"metadata":{},"output_type":"execute_result"}],"source":["fusion_val[0].shape"]},{"cell_type":"code","execution_count":null,"id":"06aeda67","metadata":{"id":"06aeda67"},"outputs":[],"source":["batch_size = 4"]},{"cell_type":"code","execution_count":null,"id":"d4419f5a","metadata":{"id":"d4419f5a"},"outputs":[],"source":["fusion_train_loader = DataLoader(fusion_train_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression_bert())\n","fusion_val_loader = DataLoader(fusion_val_dataset, batch_size = batch_size, drop_last = False, shuffle = True, collate_fn = PadSequence_regression_bert())"]},{"cell_type":"code","execution_count":null,"id":"46b18677","metadata":{"id":"46b18677","outputId":"54ab32d9-0dff-490a-9210-73ff83a465a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["sbert_GRU_regression(\n","  (gru): GRU(791, 1024, batch_first=True)\n","  (dropout): Dropout(p=0, inplace=False)\n","  (linear_1): Linear(in_features=1024, out_features=512, bias=True)\n","  (output): Linear(in_features=512, out_features=1, bias=True)\n",")\n"]}],"source":["input_dim = 791\n","hidden_dim = 1024\n","linear_dim_1 = 512\n","linear_dim_2 = linear_dim // 2\n","\n","model = sbert_GRU_regression(input_dim, hidden_dim, linear_dim_1, linear_dim_2).to(device)\n","# model.init_weights()\n","print(model)"]},{"cell_type":"code","execution_count":null,"id":"2ae0d2ae","metadata":{"id":"2ae0d2ae"},"outputs":[],"source":["loss_fn = torch.nn.MSELoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 1e-6)"]},{"cell_type":"code","execution_count":null,"id":"221d2d5c","metadata":{"id":"221d2d5c"},"outputs":[],"source":["epochs = 20\n","dev_min_loss = np.inf\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    dev_loss = 0.0\n","    batch = 0\n","    for inputs, target, lens in fusion_train_loader:\n","        print(batch, end = \"\\r\")\n","        batch += 1\n","        inputs, target = inputs.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(inputs, lens)\n","        output = torch.squeeze(output.view(-1, output.shape[-1]))\n","        target = target.view(-1)\n","        loss = torch.sqrt(loss_fn(output, target))\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","    model.eval()\n","    for inputs, target, lens in fusion_val_loader:\n","        inputs, target = inputs.to(device), target.to(device)\n","        inputs, target = inputs, target\n","        with torch.no_grad():\n","            output = model(inputs, lens)\n","            output = torch.squeeze(output.view(-1, output.shape[-1]))\n","            target = target.view(-1)\n","            loss = torch.sqrt(loss_fn(output, target))\n","            dev_loss += loss.item()\n","    \n","    train_loss = train_loss / len(sbert_train_loader_reg)\n","    dev_loss = dev_loss / len(sbert_val_loader_reg)\n","#     scheduler.step()\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tDev Set Loss: {:.6f}'.format(epoch+1, train_loss,dev_loss))\n","    if dev_loss <= dev_min_loss:\n","        print('Dev Set Loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(dev_min_loss, dev_loss))\n","        torch.save(model.state_dict(), 'model/sbert_GRU_reg.pt')\n","        dev_min_loss = dev_loss "]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"playground.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}