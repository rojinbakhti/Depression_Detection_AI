{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"general_audio_model_train.ipynb","provenance":[{"file_id":"1VlfFRD0LlB6_6_DjUT71MbE_ofDCHcPY","timestamp":1651181102379},{"file_id":"1vC_cCK9QYXUf2EzcabsikcdLCkQxor7R","timestamp":1648509584495},{"file_id":"1riOkABLGm4e2uWTB8WNOSxkzEOOQ0VUL","timestamp":1648509128576}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import sys\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/CS535/Sukavanan_Fusion\")\n","print(f\"This is my current working directory --> {os.getcwd()}\")\n","import pandas as pd\n","import numpy as np\n","import io\n","import tarfile\n","import csv\n","\n","\n","\n","# global variables\n","SEQ_LENGTH=20*60*100\n","BATCH_SIZE=15"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjXVi2owanvs","executionInfo":{"status":"ok","timestamp":1652075611641,"user_tz":420,"elapsed":25492,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"b2131385-c27f-4c7c-8df3-0eef92fadfc8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","This is my current working directory --> /content/drive/MyDrive/Colab Notebooks/CS535/Sukavanan_Fusion\n"]}]},{"cell_type":"code","source":["import pickle\n","# Read metadata (dev+train) labels\n","labels = pd.read_csv('metadata_mapped.csv')\n","\n","bin_series= pd.Series(labels['PHQ_Binary'].values, index=labels['Participant_ID'])\n","reg_series= pd.Series(labels['PHQ_Score'].values, index=labels['Participant_ID'])"],"metadata":{"id":"q3LzOeLRb3k8","executionInfo":{"status":"ok","timestamp":1652075776959,"user_tz":420,"elapsed":627,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Read test labels\n","test_labels = pd.read_csv('test_split.csv')\n","\n","test_bin_series= pd.Series(test_labels['PHQ_Binary'].values, index=test_labels['Participant_ID'])\n","test_reg_series= pd.Series(test_labels['PHQ_Score'].values, index=test_labels['Participant_ID'])"],"metadata":{"id":"Vl2qQH7yi-i4","executionInfo":{"status":"ok","timestamp":1652076335143,"user_tz":420,"elapsed":634,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Read train labels\n","import pickle\n","\n","train_labels = pd.read_csv('train_split.csv')\n","\n","train_bin_series= pd.Series(train_labels['PHQ_Binary'].values, index=train_labels['Participant_ID'])\n","train_reg_series= pd.Series(train_labels['PHQ_Score'].values, index=train_labels['Participant_ID'])"],"metadata":{"id":"Pd2axpSPhe1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read dev labels\n","dev_labels = pd.read_csv('dev_split.csv')\n","\n","dev_bin_series= pd.Series(dev_labels['PHQ_Binary'].values, index=dev_labels['Participant_ID'])\n","dev_reg_series= pd.Series(dev_labels['PHQ_Score'].values, index=dev_labels['Participant_ID'])\n"],"metadata":{"id":"XxZiLbXNpRG7","executionInfo":{"status":"ok","timestamp":1652050691768,"user_tz":420,"elapsed":236,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Total number of depressed in dev+train : 49\n","# Total number of not-depressed in dev+train: 170\n","\n","# Total number of depressed in dev: 12\n","# Total number of not-depressed in dev: 44\n","\n","# Total number of depressed in train: 37\n","# Total number of not-depressed in train: 126\n","\n","# Total number of depressed in test:\n","# Total number of not-depressed in test:"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCDD_JIYK-fT","executionInfo":{"status":"ok","timestamp":1651199782844,"user_tz":420,"elapsed":286,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"912893e0-ee0a-4e7f-cb06-adb97e0c021a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PHQ_Binary\n","0    170\n","1     49\n","dtype: int64"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["df_test = df.loc[df['patient_id'].isin(test_labels['Participant_ID'])]"],"metadata":{"id":"bXxqtfQ39DxX","executionInfo":{"status":"ok","timestamp":1652051223181,"user_tz":420,"elapsed":866,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"labeled_vgg16_combined.csv\")"],"metadata":{"id":"DGRUJ5holPBW","executionInfo":{"status":"ok","timestamp":1652075961706,"user_tz":420,"elapsed":179054,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df.groupby('patient_id').size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UFIR_1lZ_3w","executionInfo":{"status":"ok","timestamp":1652076104125,"user_tz":420,"elapsed":354,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"d7ba08c5-d153-4a50-aa8a-13aa15e02b92"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["patient_id\n","300     648\n","301     823\n","302     758\n","303     985\n","304     788\n","       ... \n","698    1441\n","702     730\n","703     776\n","707     862\n","713     790\n","Length: 219, dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df1 = df.loc[df['patient_id'][:100]]"],"metadata":{"id":"qc8wq42baRel","executionInfo":{"status":"ok","timestamp":1652076252549,"user_tz":420,"elapsed":240,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["del df"],"metadata":{"id":"qxqKfYdcbA-W","executionInfo":{"status":"ok","timestamp":1652076325719,"user_tz":420,"elapsed":3,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"labeled_vgg16_combined_test.csv\")"],"metadata":{"id":"pNEvuu0A3-E0","executionInfo":{"status":"ok","timestamp":1652055093296,"user_tz":420,"elapsed":42879,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["def padd_dataset(df,bin_series): \n","  df_padd=pd.DataFrame(np.zeros((len(bin_series)*SEQ_LENGTH, df.shape[1])), columns=df.columns)\n","\n","  for i, id in enumerate(list(set(df['patient_id']))):\n","      curr_df=df.loc[df['patient_id']==id].iloc[:SEQ_LENGTH]\n","      curr_nrow= curr_df.shape[0] if curr_df.shape[0]<=SEQ_LENGTH else SEQ_LENGTH\n","\n","      df_padd.iloc[(i+1)*SEQ_LENGTH-curr_nrow: (i+1)*SEQ_LENGTH]= curr_df\n","  print(df_padd.head(3))\n","  pi_series = []\n","  for pi_num in list(set(df['patient_id'])):\n","      pi_series+= [pi_num]*SEQ_LENGTH\n","\n","  pi_series = pd.Series(pi_series)\n","  df_padd['patient_id'] = pi_series\n","  return df_padd\n"],"metadata":{"id":"KOTx5josmdlU","executionInfo":{"status":"ok","timestamp":1652076315319,"user_tz":420,"elapsed":231,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["df_padd = padd_dataset(df1,test_bin_series)"],"metadata":{"id":"6uc4GsYG74Ot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_padd.to_csv(\"updated_test_mfcc_df_padd.csv\")"],"metadata":{"id":"a6w_HAYaAnZ8","executionInfo":{"status":"ok","timestamp":1652054900644,"user_tz":420,"elapsed":242152,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# DATALOADER\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class MyDataset_bin(Dataset):\n","\n","    def __init__(self, df_padd):\n","        self.df_padd = df_padd\n","\n","    def __len__(self):\n","        return self.df_padd.shape[0]//SEQ_LENGTH\n","\n","    def __getitem__(self, idx):\n","        drop_cols=['Unnamed: 0', 'name', 'frameTime', 'patient_id', 'bin','req']\n","        curr_x= self.df_padd.iloc[idx*SEQ_LENGTH: (idx+1)*SEQ_LENGTH].drop(drop_cols, axis=1)\n","        #print(self.df_padd.iloc[idx*SEQ_LENGTH: (idx+1)*SEQ_LENGTH]['bin'])\n","        curr_y = np.array(int(self.df_padd.iloc[idx*SEQ_LENGTH: (idx+1)*SEQ_LENGTH]['bin'].iloc[0]))\n","        #print(\"this is curr_y \", curr_y)\n","        X=torch.from_numpy(curr_x.to_numpy()).float()\n","        y=torch.from_numpy(curr_y)\n","\n","        return X, y"],"metadata":{"id":"6d749Vl1_MAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDataset_reg(Dataset):\n","\n","    def __init__(self, df_padd):\n","        self.df_padd = df_padd\n","\n","    def __len__(self):\n","        return self.df_padd.shape[0]//SEQ_LENGTH\n","\n","    def __getitem__(self, idx):\n","        drop_cols=['Unnamed: 0', 'name', 'frameTime', 'patient_id', 'bin','req']\n","        curr_x= self.df_padd.iloc[idx*SEQ_LENGTH: (idx+1)*SEQ_LENGTH].drop(drop_cols, axis=1)\n","        #print(self.df_padd.iloc[idx*SEQ_LENGTH: (idx+1)*SEQ_LENGTH]['bin'])\n","        curr_y = np.array(int(self.df_padd.iloc[idx*SEQ_LENGTH: (idx+1)*SEQ_LENGTH]['req'].iloc[0]))\n","        #print(\"this is curr_y \", curr_y)\n","        X=torch.from_numpy(curr_x.to_numpy()).float()\n","        y=torch.from_numpy(curr_y)\n","\n","        return X, y"],"metadata":{"id":"OIukWD0HsPIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Total egemaps datapoints (dev+train)\n","df = pd.read_pickle('labeled_mfcc_combined.pkl')"],"metadata":{"id":"j0Hmj2GcrJEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mfcc_df = df"],"metadata":{"id":"co4HcYI9tFLb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting only train egemaps datapoints\n","train_mfcc_df  = mfcc_df[mfcc_df['patient_id'].isin(train_labels['Participant_ID'])]"],"metadata":{"id":"LO2xcTAorXtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_mfcc_df.head(50)"],"metadata":{"id":"LDX8tC1ntLt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extracting only dev egemaps datapoints\n","dev_mfcc_df  = mfcc_df[egemaps_df['patient_id'].isin(dev_labels['Participant_ID'])]"],"metadata":{"id":"nCHidhbgr7MP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# create Tensor datasets\n","train_data = MyDataset_reg(train_egemaps_df) #this is the train data \n","valid_data = MyDataset_reg(dev_egemaps_df) #this is the dev data which will be used for validation\n","\n","# THIS IS DATALOADER FOR BINARY \n","\n","# train_data = MyDataset_bin(train_egemaps_df) #this is the train data \n","# valid_data = MyDataset_bin(dev_egemaps_df) #this is the dev data which will be used for validation\n","\n","\n","# valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","# test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n","\n","\n","train_loader = DataLoader(train_data, shuffle=False, batch_size=BATCH_SIZE,drop_last=True)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE,drop_last=True)\n","# test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)"],"metadata":{"id":"MTFTQ-kyCFsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for X, y in train_loader:\n","    print(\"X: \", X.shape, ' y: ', len(y))\n","    print(\"---\")\n","    print(y)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EarWHmxGERuz","executionInfo":{"status":"ok","timestamp":1651253114751,"user_tz":420,"elapsed":1749,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"2d1886a0-1408-4dec-960a-435888378898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 4, 2, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([10,  0,  0,  0,  0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0,  0, 22,  0,  0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([7, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 7, 4])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 1, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0,  0,  0, 10,  0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0,  0,  0,  0, 23])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 5, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0,  0, 19,  7,  0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0, 13,  0,  0, 16])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0,  0, 10,  7,  0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 9])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0,  0,  0,  0, 17])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 3, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([ 0,  0,  0, 12,  0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 2, 0, 2, 1])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([19,  0,  7,  0,  0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 9, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 9, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 0, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([0, 2, 0, 0, 0])\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","tensor([15,  0,  0,  0,  0])\n"]}]},{"cell_type":"code","source":["for X, y in valid_loader:\n","    print(\"X: \", X.shape, ' y: ', len(y))\n","    print(\"---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fc1hozhujjwx","executionInfo":{"status":"ok","timestamp":1651253117010,"user_tz":420,"elapsed":624,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"7b4ee5be-c96f-4322-fe50-e09cbfb0dc3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n","X:  torch.Size([5, 120000, 23])  y:  5\n","---\n"]}]},{"cell_type":"code","source":["# First checking if GPU is available\n","train_on_gpu=torch.cuda.is_available()\n","\n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVmZjzmxJzX0","executionInfo":{"status":"ok","timestamp":1651252505125,"user_tz":420,"elapsed":311,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"804a7209-e954-4820-9b0b-619c354eef44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on GPU.\n"]}]},{"cell_type":"code","source":["# obtain one batch of training data\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","\n","print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","print()\n","print('Sample label size: ', sample_y.size()) # batch_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIOyToEnOmS6","executionInfo":{"status":"ok","timestamp":1651243852003,"user_tz":420,"elapsed":268,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"f09ea4dd-e266-4def-c2ee-dbdae6d992ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample input size:  torch.Size([15, 120000, 23])\n","\n","Sample label size:  torch.Size([15])\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, n_layers, output_size, drop_prob):\n","        super(RNN, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        self.drop_prob=drop_prob\n","        self.dropout = nn.Dropout(p=self.drop_prob)\n","\n","        self.output_size = output_size\n","        \n","        \n","        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.n_layers, \n","                            dropout=0.3, batch_first=True)\n","        \n","\n","        \n","        self.fc = nn.Linear(self.hidden_dim, self.output_size)\n","        self.sig = nn.Sigmoid()\n","        self.RELU = nn.ReLU()\n","        \n","\n","    def forward(self, x, hidden):\n","        batch_size = x.size(0)\n","\n","        #x = x.long()\n","        lstm_out, hidden = self.lstm(x, hidden)\n","        #print(lstm_out)\n","        lstm_out = lstm_out[:, -1, :] # getting the last time step output\n","        \n","        out = self.dropout(lstm_out)\n","\n","        #print(\"LSTM_out\",out)\n","        out = self.fc(out)\n","        #print(out)\n","        out = self.RELU(out)\n","        #out = self.sig(out)\n","        \n","        return out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden"],"metadata":{"id":"ZQeQxVRJOmrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","input_dim = 23\n","hidden_dim = 64\n","n_layers = 1\n","output_size = 1\n","drop_prob=0.2\n","\n","net = RNN(input_dim, hidden_dim, n_layers, output_size, drop_prob)\n","\n","print(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T7pbaWGDQKFK","executionInfo":{"status":"ok","timestamp":1651254336805,"user_tz":420,"elapsed":269,"user":{"displayName":"Rojin Bakhti","userId":"04704722990124402478"}},"outputId":"b530bca6-a3a9-41c0-cff8-71bea505722e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RNN(\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (lstm): LSTM(23, 64, batch_first=True, dropout=0.3)\n","  (fc): Linear(in_features=64, out_features=1, bias=True)\n","  (sig): Sigmoid()\n","  (RELU): ReLU()\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","source":["# h = net.init_hidden(BATCH_SIZE)\n","# h = tuple([each.data for each in h])\n","# if(train_on_gpu):\n","#     sample_x, sample_y = sample_x.cuda(),sample_y.cuda()\n","# net = net.cuda()\n","# out,h = net.forward(sample_x,h)"],"metadata":{"id":"dyXeVBK2QOJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2JcrI1XKOmxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"66hFTtZ_49uf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","lr=0.0001\n","\n","#criterion = costum_loss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","loss_fn = torch.nn.MSELoss()\n","epochs = 20\n","\n","counter = 0\n","print_every = 5\n","clip=5 # gradient clipping\n","criterion = torch.nn.MSELoss()\n","if(train_on_gpu):\n","    net.cuda()\n","train_loss = 0.\n","dev_loss = 0.\n","net.train()\n","val_h = net.init_hidden(BATCH_SIZE)\n","for e in range(epochs):\n","\n","    h = net.init_hidden(BATCH_SIZE)\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","\n","        h = tuple([each.data for each in h])\n","\n","        net.zero_grad()\n","\n","        output, h = net(inputs, h)\n","        print(output)\n","        loss = loss_fn(output.squeeze(), labels.float())\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","        train_loss += loss.item()\n","        \n","        if counter % print_every == 0:\n","            \n","            dev_losses = []\n","            net.eval()\n","            for inputs, labels in valid_loader:\n","                val_h = tuple([each.data for each in val_h])\n","                if(train_on_gpu):\n","                    inputs, labels = inputs.cuda(), labels.cuda()\n","                with torch.no_grad():\n","                  output, val_h = net(inputs, val_h)\n","                  dev_loss += torch.sqrt(loss_fn(output.squeeze(), labels.float()))\n","                  dev_loss += loss.item()\n","\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Train Loss: {:.6f}...\".format(train_loss/len(train_loader)),\n","                  \"Train_loder_size {:.6f}\".format(len(train_loader)),\n","                  \"Dev Loss: {:.6f}\".format(dev_loss/len(valid_loader)))"],"metadata":{"id":"kn6-p3X2Om0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()"],"metadata":{"id":"cm96HtCWTmJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"C699g7z-2pNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"49YYbTwFWbq6"},"execution_count":null,"outputs":[]}]}