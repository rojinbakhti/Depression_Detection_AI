# Depression_Detection_AI

<img width="729" alt="image" src="https://user-images.githubusercontent.com/59149638/184985107-f4180428-0a41-4679-9414-4644b42edf91.png">


## Project Description
In this project, we propose a method to detect depression among patients using audio and text modalities and perform an analysis of the performance of different modalities. First, we started the porject by developing unimodal models using the audio and text input features and then develop multimodal models using the best performing unimodal models by applying early, late, and hybrid fusion techniques. Later, we compare the performance improvement, if any, going from unimodal to multimodal (in this case bimodal) models. The data used in this project is the Extended Distress Analysis Interview Corpus (E – DAIC), which is provided as part of the Audio/Visual Emotion Challenge and Workshop (AVEC 2019).

## Data
The Extended Distress Analysis Interview Corpus (E-DAIC) is an extended version of the DAIC – Wizard-of-Oz corpus (DAIC- WOZ). The E-DAIC dataset contains semi-clinical interviews, designed to diagnose mental health disorders such as anxiety, depression and Post-Traumatic Stress Disorder (PTSD). These interviews were collected as part of a large effort to create a computer agent that interviews people and identifies verbal and nonverbal indicators of mental illnesses.

## Results
Check out the project proposal [here](https://link-url-here.org) and the presentation [here](https://link-url-here.org).
